{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755124de-77b7-4800-9692-f51463671c66",
   "metadata": {},
   "source": [
    "# Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f2468a-519c-451b-a9cd-817fef4d9942",
   "metadata": {},
   "source": [
    "Memoria realizada por Lola Conde Herrera para la asignatura Visión Artificial.\n",
    "\n",
    "A efectos de facilitar la revisión de los ejercicios, se realizó un documento para cada uno de ellos, por lo que es posible consultarlos de manera separada. Se adjunta en aquí el contenido de dichos documentos, para que pueda estar contenido en una sola memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e80ecc-d61e-468f-b9fa-bd9b937d014e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7d54a-6e4b-4709-8c36-708fd2328599",
   "metadata": {},
   "source": [
    "**Obligatorios:**\n",
    "\n",
    "[HANDS](#HANDS)\n",
    "\n",
    "[FILTROS](#FILTROS)\n",
    "\n",
    "[CLASIFICADOR y SIFT](#CLASIFICADOR-y-SIFT)\n",
    "\n",
    "[RECTIF](#RECTIF)\n",
    "\n",
    "[RA](#RA)\n",
    "\n",
    "**Opcionales**\n",
    "\n",
    "[MAPA](#MAPA)\n",
    "\n",
    "[FILTROS II](#FILTROS-II)\n",
    "\n",
    "[POLYGON](#POLYGON)\n",
    "\n",
    "[MODEL3D](#MODEL3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12072a20-a842-4434-9f21-d9e099e94f5b",
   "metadata": {},
   "source": [
    "# HANDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feec721-ff27-4d0e-8e7f-a7ed803e7676",
   "metadata": {},
   "source": [
    "## Enunciado\n",
    "\n",
    "Amplia el ejemplo `code/DL/hands/mano.py` hecho en clase para reconocer gestos simples, como por ejemplo contar el n´umero de dedos extendidos. Haz un controlador sin contacto de varios grados de libertad que mida, al menos, distancia de la mano a la cámara y ángulo de orientación. Utilízalo para controlar alguno de tus programas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d9519-c90e-44ae-bbb5-5f9e7c2ac9c5",
   "metadata": {},
   "source": [
    "## Solución\n",
    "\n",
    "La solución está en `HANDS/mano.py`. La aplicación que controla está en `HANDS/programa.py`.\n",
    "\n",
    "Este programa realiza las siguientes tareas:\n",
    "\n",
    "- **Mostrar una cámara**\n",
    "\n",
    "  Se captura cada fotograma, se procesa para realizar las tareas indicadas posteriormente, y se muestra.\n",
    "\n",
    "- **Dibujar la palma de la mano y los dedos**\n",
    "\n",
    "  Obteniendo los puntos de la mano con la herramienta `MediaPipe Hands`, se han realizado líneas entre el principio y el final de los dedos de la mano. \n",
    "\n",
    "  Para la palma, primero se ha obtenido el centro de tres puntos que se encuentran en el borde de la palma, de forma que se obtiene el centro de la palma. En segundo lugar, se obtiene el radio, calculado como la distancia entre el centro y un punto que está al borde de la palma. De esta forma, se puede dibujar un círculo en la palma con el centro y radio calculados.\n",
    "\n",
    "- **Calcular qué dedos hay levantados**\n",
    "\n",
    "  Para los dedos índice, corazón, anular y meñique, se ha calculado si se encuentran abiertos o cerrados usando la distancia entre el punto más bajo de la palma y el principio o el final del dedo. Si el dedo se encuentra cerrado, la distancia entre el punto más bajo de la palma y el final del dedo será menor que la distancia entre el punto más bajo de la palma y el principio del dedo. Si está abierto, es al contrario (es mayor).\n",
    "\n",
    "  Para el dedo pulgar, se ha considerado abierto si el principio del índice está más cerca del final de la mano que el final del pulgar. Si está más lejos, el dedo pulgar está cerrado.\n",
    "\n",
    "- **Calcular el ángulo de orientación de la mano**\n",
    "\n",
    "  Para calcular el ángulo de orientación de la mano, se ha calculado el ángulo entre dos vectores, el vector horizontal y el vector que se encuentra entre el final de la palma y el principio del dedo corazón. El segundo vector debe formar 0 grados con el horizontal si la mano está horizontal, y si esta empieza a girar, se calcula el ángulo.\n",
    "\n",
    "  Si el principio de la palma está más a la izquierda que el principio del dedo corazón, entonces el ángulo es negativo. Esto se realiza comparando la componente x de los puntos.\n",
    "\n",
    "- **Controlar una aplicación**\n",
    "\n",
    "  Si todos los dedos están levantados, se abre la aplicación, y si todos están cerrados se cierra. La condición ya estaba calculada en el cálculo de los dedos levantados.\n",
    "\n",
    "- **Calcular la distancia de la mano a la cámara**\n",
    "\n",
    "  Se usa la palma de la mano para calcular la distancia. Se calcula con la fórmula siguiente:\n",
    "\n",
    "  $$\\frac{\\text{distancia focal} * \\text{ancho de la palma en centímetros}}{\\text{ancho de la palma en píxeles}} = \\text{distancia a la cámara en centímetros}$$\n",
    "\n",
    "  Para calcular la distancia focal, se ha utilizado la herramienta `calibrate.py`, que se puede encontrar en [el material de la asignatura](https://github.com/albertoruiz/umucv/tree/master/code/calibrate). Las fotografías tomadas para el cálculo de la distancia focal se encuentran en la carpeta `HANDS/pattern`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651a962-a4f5-4f43-85a0-d3981ca9321b",
   "metadata": {},
   "source": [
    "## Explicación de la fórmula para el cálculo de la distancia de la mano a la cámara\n",
    "\n",
    "La distancia focal es la distancia (en píxeles) desde el centro del objetivo de la cámara hasta el sensor de esta. El objetivo es el lugar donde se encuentra la lente de la cámara, y el sensor es donde se proyecta la luz de la imagen para poder procesarla o almacenarla.\n",
    "\n",
    "La distancia focal se obtiene realizando la calibración de la cámara. Su obtención es muy útil, debido a que con esta se puede calcular la distancia de la cámara a un objeto (dada una fotografía), si se sabe el tamaño de este en la realidad y en la fotografía.\n",
    "\n",
    "Para explicar la fórmula para obtener la distancia, se necesita un objeto. Es este caso, se va a explicar con un esquema cuyo objeto es una botella:\n",
    "\n",
    "<img src=\"HANDS/img/formula1.jpeg\" style=\"width:20%\"/>\n",
    "\n",
    "Como se puede observar: X es la altura de la botella en el mundo real, Z es la distancia de la cámara a la botella, x es la altura de la botella en la fotografía tomada, y f es la distancia focal.\n",
    "\n",
    "Se puede observar como se forman dos triángulos (uno con lados f y x, y otro con lados X y Z). Los triángulos se encuentran en posición de Thales, por lo que se obtiene la siguiente fórmula:\n",
    "\n",
    "$$\\frac{X}{Z}=\\frac{x}{f}$$\n",
    "\n",
    "Si se quiere obtener más información sobre el teorema de Thales, se puede observar la siguiente url: https://www.superprof.es/apuntes/escolar/matematicas/geometria/basica/triangulos-en-posicion-de-thales.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bba37-2ccf-439f-8f60-11bd2d07906a",
   "metadata": {},
   "source": [
    "## Ejecución del código: Ejemplo de funcionamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810390fd-5902-4fdd-a720-fd4081958175",
   "metadata": {},
   "source": [
    "Para ejecutar el código de HANDS, se debe de ejecutar el código de `mano.py` desde la carpeta `HANDS` en el entorno de anaconda prompt explicado al inicio de la asignatura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31817c1d-50c0-4112-8666-58291ca233ca",
   "metadata": {},
   "source": [
    "Una vez abierta la aplicación, se debe mostrar una mano en la cámara (si se muestra más de una, sólo se tendrá una en cuenta, y en esta se dibujarán líneas en los dedos y un circulo azul en la palma)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f1f6d-82f6-481f-bbff-0aa809a15371",
   "metadata": {},
   "source": [
    "- **Dedos levantados**:\n",
    "\n",
    "  Si se levantan o bajan los dedos, se indica en la pantalla qué dedos están levantados:\n",
    "  \n",
    "  <table><tr>\n",
    "  <td> <img src=\"HANDS/img/dedos-en-alto.png\"/> </td>\n",
    "  <td> <img src=\"HANDS/img/4-dedos-en-alto.png\"/> </td>\n",
    "  <td> <img src=\"HANDS/img/dedos-hacia-delante.png\"/> </td>\n",
    "  <td> <img src=\"HANDS/img/dedos-hacia-atras.png\"/> </td>\n",
    "  </tr></table>\n",
    "\n",
    "  Como se puede observar, el programa diferencia los dedos indiferentemente de la posición de la mano.\n",
    "\n",
    "- **Grados**:\n",
    "\n",
    "  También se puede observar el ángulo de la mano, de forma que va desde 180 hasta -180 grados:\n",
    "\n",
    "  <table><tr>\n",
    "  <td> <img src=\"HANDS/img/0-grados.png\"/> </td>\n",
    "  <td> <img src=\"HANDS/img/90-grados.png\"/> </td>\n",
    "  <td> <img src=\"HANDS/img/179-grados.png\"/> </td>\n",
    "  <td> <img src=\"HANDS/img/-90-grados.png\"/> </td>\n",
    "  </tr></table>\n",
    "\n",
    "- **Distancia a la cámara**:\n",
    "\n",
    "  Como se puede observar, se indica la distancia a la cámara (se muestra un video donde se acerca y se aleja la mano para que se va de mejor forma:\n",
    "\n",
    "  <video src=\"HANDS/img/distancia-mano.mp4\" controls='play' style=\"width:50%\">\n",
    "  </video>\n",
    "\n",
    "- **Abrir y cerrar una aplicación**:\n",
    "\n",
    "  Se presenta un vídeo en el que se ve cómo se abre (cinco dedos arriba) y se cierra (cinco dedos abajo) una aplicación con `mano.py`:\n",
    "\n",
    "  <video src=\"HANDS/img/abrir-cerrar-aplicacion.mp4\" controls='play' style=\"width:50%\">\n",
    "  </video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fffdd4-c3d3-4002-a1d6-5ab8b4584822",
   "metadata": {},
   "source": [
    "## Tardanza del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a3c78-fb05-423d-835a-ff77a13b6230",
   "metadata": {},
   "source": [
    "Para ver cuanto tarda en ejecutar el código se va a añadir las siguientes líneas de código cuando se captura y cuando se muestra el frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b4256c-4a88-4568-b4c3-e409dde9cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# ...\n",
    "inicio = time.time()\n",
    "\n",
    "# Código que se hace cada frame\n",
    "\n",
    "fin = time.time()\n",
    "print(fin-inicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cbbc81-82b0-476e-8a68-236480bfb3ba",
   "metadata": {},
   "source": [
    "Como se puede observar, este código tarda 0 segundos porque no hace nada en medio, pero al añadirlo a `mano.py`, el tiempo aumenta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843c1a7-4ed5-47c6-84fb-bf54718c583e",
   "metadata": {},
   "source": [
    "- Al no mostrar manos, cada frame tarda unos 0.07 segundos en mostrarse.\n",
    "\n",
    "- Como es de esperar, al mostrar una mano, el hecho de tener que hacer más cálculos hace que el frame tarde más en mostrarse (unos 0.12 segundos).\n",
    "\n",
    "- El hecho de mover las manos hace que en algunos frames sea más dificil calcular el frame resultante a mostrar (0.19 segundos), y en otros más sencillo (0.06 segundos). Los frames más sencillos tienen un tiempo similar a los tiempos al no mostrar manos, por lo que se puede intuir que en esos frames no se mostraron manos (o estas no fueron reconocidas al no verse de forma correcta por la velocidad)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4cf07-4ecd-49f2-ae4b-dc378bea5182",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Bibliografía usada\n",
    "\n",
    "[Comprender Hand Landmark - MediaPipe](https://github.com/google/mediapipe/blob/master/docs/solutions/hands.md)\n",
    "\n",
    "[Aprender a dibujar cuadrados](https://docs.opencv.org/3.4/d6/d6e/group__imgproc__draw.html#ga07d2f74cadcf8e305e810ce8eed13bc9)\n",
    "\n",
    "[Aprender a escribir texto](https://docs.opencv.org/3.4/d6/d6e/group__imgproc__draw.html#ga5126f47f883d730f633d74f07456c576)\n",
    "\n",
    "[Aprender a dibujar círculos: cv.circle](https://docs.opencv.org/3.4/d6/d6e/group__imgproc__draw.html#gaf10604b069374903dbd0f0488cb43670)\n",
    "\n",
    "[Funciones de math](https://docs.python.org/3/library/math.html)\n",
    "\n",
    "[Ángulo entre dos vectores (para calcular la orientación de la mano)](https://www.superprof.es/apuntes/escolar/matematicas/analitica/vectores/angulo-de-dos-vectores.html)\n",
    "\n",
    "[Material de la asignatura](https://github.com/albertoruiz/umucv/blob/master/notebooks/imagen.ipynb)\n",
    "\n",
    "[Entender qué es la distancia focal](https://www.sony.es/electronics/support/articles/00267921)\n",
    "\n",
    "[Entender qué es un sensor](https://www.blogdelfotografo.com/tipos-caracteristicas-ventajas-sensores-camaras-fotos/)\n",
    "\n",
    "[Teorema de Thales](https://www.superprof.es/apuntes/escolar/matematicas/geometria/basica/triangulos-en-posicion-de-thales.html)\n",
    "\n",
    "También se ha usado ChatGPT para crear el programa llamado `programa.py`, el cual crea una ventana. Este se utilizó para controlarlo con `mano.py`, de forma que se vea la ventana abrirse y cerrarse según el gesto de la mano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377add83-b774-44f2-8d6b-e9320d58e9c9",
   "metadata": {},
   "source": [
    "# FILTROS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d2aae-7130-4d33-829c-545b2d460bbb",
   "metadata": {},
   "source": [
    "## Enunciado\n",
    "\n",
    "Muestra en vivo el efecto de diferentes filtros, seleccionando con el teclado el filtro deseado y modificando sus parámetros (p.ej. el nivel de suavizado) con trackbars. Aplica el filtro en un ROI para comparar el resultado con el resto de la imagen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9214345-0816-4d34-ab6f-0e57e38ec683",
   "metadata": {},
   "source": [
    "## Filtros disponibles\n",
    "\n",
    "- **Blanco y negro**\n",
    "\n",
    "  En primer lugar, con el uso de la librería OpenCV se pasa a gris el color de la región de interés. Esto deja la imagen con 1 canal.\n",
    "\n",
    "  En segundo lugar, se vuelve a pasar a BGR porque este es el formato de la imagen completa. Al sobrescribir la región de interés (seleccionada) con el filtro correspondiente, lo que se escribe debe tener el mismo formato que el resto de la imagen (del fotograma).\n",
    "\n",
    "- **Filtro box**\n",
    "\n",
    "  Se utiliza una función de OpenCV que lo realiza. Esta función utiliza un kernel de todos unos para que cada píxel se transforme en una media entre sus vecinos, consiguiendo así un efecto de emborronamiento.\n",
    "\n",
    "- **Filtro gaussiano**\n",
    "\n",
    "  Se utiliza una función de OpenCV que lo realiza.\n",
    "\n",
    "  Una función gaussiana tiene una forma tal que los valores centrales tienen un mayor valor en la componente y que los lejanos al centro. El filtro gaussiano hace algo parecido, de forma que tiene valores mayores en el centro de la matriz utilizada, y valores cada vez más pequeños conforme se aleja del centro de la matriz.\n",
    "\n",
    "  Se puede indicar el tamaño de la desviación estándar en x. Si esta es muy pequeña, el filtro no tendrá mucho efecto. Si es muy grande, la imagen se verá muy emborronada.\n",
    "\n",
    "  Esto es beneficioso para suavizar imágenes y reducir el ruido sin perder demasiada información de detalle. Al contrario que ocurría con el filtro box, que podía añadir ruido a la imagen, el filtro gaussiano no añade ruido.\n",
    "\n",
    "- **Filtro de mediana**\n",
    "\n",
    "  Se utiliza una función de OpenCV.\n",
    "\n",
    "  Este filtro hace que cada pixel se transforme en la mediana de sus vecinos y él mismo.\n",
    "\n",
    "  También se utiliza para reducir el ruido, eliminando los puntos aislados de alta o baja intensidad.\n",
    "\n",
    "- **Filtro bilateral**\n",
    "\n",
    "  Se utiliza una función de OpenCV que lo realiza.\n",
    "\n",
    "  Tal y como indica la [documentación de OpenCV](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9), este filtro es muy util para reducir el ruido mientras se mantiene la nitidez de los bordes. El inconveniente es que es más lento que la mayoría de los otros filtros.\n",
    "\n",
    "  Este filtro utiliza dos filtros gaussianos, uno hace que los píxeles cercanos tengan más peso que los lejanos, y el otro hace que los píxeles con valores de intensidad similares tengan más peso. \n",
    "\n",
    "  De esta forma, se consigue que sólo se escojan para el suavizado los píxeles cercanos y con valores similares, lo que hace que los bordes se mantengan nítidos.\n",
    "\n",
    "- **Filtro del mínimo**\n",
    "\n",
    "  Se utiliza una función de la librería SciPy.\n",
    "\n",
    "  Este filtro hace que cada pixel se transforme en el valor mínimo de sus vecinos y él mismo. La cantidad de vecinos viene determinada por un parámetro pasado como argumento a la función.\n",
    "\n",
    "\n",
    "- **Filtro del máximo**\n",
    "\n",
    "  Se utiliza una función de la librería SciPy.\n",
    "\n",
    "  Este filtro hace lo contrario que el anterior, ya que en lugar de tomar el valor mínimo de sus vecinos, toma el valor máximo.\n",
    "\n",
    "  La cantidad de vecinos también viene determinada por un parámetro pasado como argumento a la función.\n",
    "\n",
    "- **Transformación de valor**\n",
    "\n",
    "  Los píxeles de las imágenes pueden ser modificados individualmente sin tener en cuenta su entorno. En este caso, la modificación implica el aumento constante de la luminancia de los píxeles, lo que resulta en un aclarado u oscurecimiento de la imagen.\n",
    "\n",
    "- **Ecualizador del histograma**\n",
    "\n",
    "  Se utiliza una función de OpenCV que lo realiza.\n",
    "\n",
    "  El histograma de una imagen es una representación gráfica de los valores de los píxeles de esta. En este caso, se ha querido ecualizar los valores de luminancia de la imagen, transformando la distribución de los valores para abarcar todo el rango de valores posibles\n",
    "\n",
    "  De esta forma, se se aumenta el contraste de la imagen y haciendo que los detalles sean más visibles. Estos detalles antes podían haber estado ocultos por sobreexposición o subexposición.\n",
    "\n",
    "  Si se quiere leer más información al respecto, se puede consultar la [documentación de OpenCV](https://docs.opencv.org/3.4/d4/d1b/tutorial_histogram_equalization.html).\n",
    "\n",
    "- **CLAHE**\n",
    "\n",
    "  Se utiliza una función de OpenCV que implementa CLAHE.\n",
    "\n",
    "  El ecualizador de histogramas global (el anterior) no consigue mejorar en gran medida las zonas de las imágenes muy claras u oscuras en relación con el resto de la imagen. Esto es debido a que el ecualizador de histogramas no tiene en cuenta la distribución de los valores de los píxeles en zonas concretas de la imagen, sino la distribución global.\n",
    "\n",
    "  Por otro lado, CLAHE utiliza varios histogramas locales, en lugar de uno global, para ecualizar la imagen. Esto permite mejorar las zonas de la imagen que antes no se podían mejorar con el ecualizador de histogramas global.\n",
    "\n",
    "  También es importante destacar que CLAHE limita el contraste de las regiones antes de realizar la ecualización, evitando así la amplificación del ruido en áreas con valores constantes.\n",
    "\n",
    "- **Opening**\n",
    "\n",
    "  Se utiliza una función de OpenCV. Si se quiere saber más, se puede consultar la [siguiente url](https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html#gsc.tab=0).\n",
    "\n",
    "  El opening es una operación morfológica que consiste en aplicar una erosión seguida de una dilatación. Esto es útil para eliminar el ruido de las imágenes.\n",
    "\n",
    "  La erosión en imágenes con solo valores 1 o 0, hace que los píxeles sólo tengan 1 si los píxeles de su alrededor sólo tienen valor 1 (el resto tienen valor 0). La dilatación en ese contexto realiza lo contrario, solo los píxeles que estén rodeados de píxeles de valor 0 se convierten en 0 (el resto son 1s).\n",
    "\n",
    "  Sin embargo, por la manera en la que se implementa en OpenCV, la erosión pone en cada píxel el valor mínimo de los que está rodeado, y la dilatación pone el valor máximo de los que está rodeado.\n",
    "\n",
    "  De esta forma, al aplicar opening a la componente de luminancia de una imagen, se consigue que los píxeles que estén rodeados de píxeles con valores de luminancia muy bajos se conviertan en píxeles con valores de luminancia muy bajos, y los píxeles que estén rodeados de píxeles con valores de luminancia muy altos se conviertan en píxeles con valores de luminancia muy altos, eliminando así los brillos blancos pequeños que puedan formarse en la imagen por la luz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca9391-8f0c-4cf8-90d8-604e5bd4d15b",
   "metadata": {},
   "source": [
    "## Organización del código\n",
    "\n",
    "- **filtroConstructor.py**\n",
    "\n",
    "  Este archivo contiene la clase ``Filtro``. Esta clase contiene:\n",
    "  - Un identificador (la tecla asociada para activarlo)\n",
    "  - Un nombre\n",
    "  - Un método para aplicar el filtro a una imagen\n",
    "  - Un método para agregar los trackbars necesarios a la ventana\n",
    "\n",
    "  De esta forma, se conoce la estructura de cada filtro. \n",
    "\n",
    "  El método para aplicar el filtro está implementado, de forma que si una subclase no lo sobrescribe, no se aplica ningún filtro y la imagen se devuelve tal cual. El método para añadir los trackbars también está implementado, de forma que si una subclase no lo sobrescribe, no se añade ningún trackbar.\n",
    "\n",
    "- **filtrosColeccion.py**\n",
    "\n",
    "  Este archivo contiene los filtros que se han añadido al programa. Cada filtro es una subclase de ``Filtro``.\n",
    "\n",
    "  De esta forma, no se necesita modificar el código principal para añadir un nuevo filtro, solo se necesita añadir una nueva subclase de ``Filtro`` en este archivo.\n",
    "\n",
    "- **filtros.py**\n",
    "\n",
    "  Este archivo contiene el código principal del programa.\n",
    "\n",
    "  En primer lugar, se crea la ayuda, que muestra las teclas que se pueden pulsar para activar los filtros, para cambiar la región de interés, para cambiar el modo de visualización, y para mostrar u ocultar la ayuda.\n",
    "\n",
    "  Las teclas que se pueden pulsar para activar los filtros son las que se han asociado a cada filtro en ``filtrosColeccion.py``. \n",
    "\n",
    "  Por otro lado, se guardan los filtros de ``filtrosColeccion.py`` en una lista.\n",
    "\n",
    "  Posteriormente, se realiza lo siguiente por cada fotograma de la cámara:\n",
    "\n",
    "  1. Se guardan las opciones seleccionadas por el usuario.\n",
    "  2. Si se ha presionado una tecla correspondiente a un filtro, y este filtro no es el que ya se está aplicando, se cambia el filtro. Si se cambia de filtro, se vuelve a crear la ventana y se añaden trackbars correpondientes al filtro seleccionado. Esto ocurre porque no se pueden eliminar los trackbars del filtro anterior salvo si se elimina y se vuelve a crear la ventana.\n",
    "  3. Si se ha presionado una tecla para visualizar solo la región de interés o todo el fotograma, o para alternar entre color y blanco y negro, se registra la opción seleccionada.\n",
    "  4. Si se presiona la tecla ``h'', se muestra u oculta la ayuda.\n",
    "  5. Se guarda la región de interés seleccionada.\n",
    "  6. Se aplica el filtro seleccionado a la sección de interés, y se escribe el nombre del filtro.\n",
    "  7. Se pasa a blanco y negro la región de interés si así se ha seleccionado.\n",
    "  8. Se dibuja un rectángulo alrededor de la sección de interés.\n",
    "  9. Se muestra el fotograma o la región de interés (según se haya seleccionado).\n",
    "\n",
    "  De esta forma, el único filtro al que se accede es al filtro NoFiltro, ya que se debe empezar el programa sin ningún filtro aplicado. El resto de filtros pueden añadirse o eliminarse, con el único detalle de que hay que fijarse en no añadir un filtro asociado a una tecla ya usada.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af226d42-e747-47b9-8af3-53c4e43afac4",
   "metadata": {},
   "source": [
    "## Ejecución del código: Ejemplo de funcionamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373d1e5-1008-4407-9d5f-0c974ba1331a",
   "metadata": {},
   "source": [
    "Para ejecutar el código de FILTROS, se debe de ejecutar el código de `filtros.py` desde la carpeta `FILTROS` en el entorno de anaconda prompt explicado al inicio de la asignatura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9e814-8853-44ac-b950-a97663c203f0",
   "metadata": {},
   "source": [
    "- **Seleccionar ROI**\n",
    "\n",
    "  Se debe de seleccionar una parte de la ventana, de forma que en esta se muestre el filtro seleccionado:\n",
    "\n",
    "  <video src=\"FILTROS/img/seleccionROI.mp4\" controls='play' style=\"width:35%\">\n",
    "  </video>\n",
    "\n",
    "  Como se puede observar, cada vez que se selecciona un filtro, se vuelve a crear la ventana.\n",
    "\n",
    "- **Algunos filtros**\n",
    "\n",
    "  Se muestra el filtro seleccionado en el \"ROI\" cuando se pulsa la tecla correspondiente al filtro, y se pueden modificar algunos parámetros de algunos filtros.\n",
    "\n",
    "  - **Ecualizador de histograma VS CLAHE**\n",
    "  \n",
    "    <video src=\"FILTROS/img/CLAHE-vs-EH.mp4\" controls='play' style=\"width:40%\">\n",
    "    </video>\n",
    "    \n",
    "    Como se puede observar, mientras que CLAHE muestra la imagen con más detalles, el ecualizador de histograma amplifica el ruido de la imagen mostrando zonas demasiado blancas en contraste con otras.\n",
    "\n",
    "  - **Opening**\n",
    "    \n",
    "    <table style=\"width:50%\"><tr>\n",
    "    <td> <img src=\"FILTROS/img/sin-filtro.png\"/> </td>\n",
    "    <td> <img src=\"FILTROS/img/openin.png\"/> </td>\n",
    "    </tr></table>\n",
    "\n",
    "    Como se puede observar, el filtro *opening* permite eliminar los brillos muy blancos que aparecen por las luces (se puede observar en los brillos de las gafas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493164d-e7c2-4296-af47-30e6bc94fd77",
   "metadata": {},
   "source": [
    "## Tardanza del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576ed01-d523-4fb1-88f1-dce1b46eca76",
   "metadata": {},
   "source": [
    "Para ver cuanto tarda en ejecutar el código se va a añadir las siguientes líneas de código al inicio del programa (antes de mostrar frames), y cuando se captura y cuando se muestra el frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96e2e1-c2a6-432a-942d-2fa0788f73cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# ...\n",
    "inicio = time.time()\n",
    "\n",
    "# Código que se hace cada frame\n",
    "\n",
    "fin = time.time()\n",
    "print(fin-inicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5ed37-011d-4499-86af-be39a45838d5",
   "metadata": {},
   "source": [
    "Como se puede observar, este código tarda 0 segundos porque no hace nada en medio, pero al añadirlo a `filtros.py`, el tiempo aumenta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b488837-13fd-46ca-9389-86ab623dba40",
   "metadata": {},
   "source": [
    "- La carga del programa tarda 0.17 segundos en ejecutarse (en efecto, sólo crea la ventana y muestra la ayuda)\n",
    "\n",
    "- **No filtro**: 0.001 segundos\n",
    "\n",
    "- **Box**: Dependiendo del tamaño del ROI, entre *0.001* (pequeño) y *0.003* (grande) segundos.\n",
    "\n",
    "- **Gaussian**: Al igual que en Box, el tamaño del ROI afecta a los segundos. A su vez, cuanto más aumenta el parámetro de desviación estándar, más tarda en ejecutarse (hasta llegar a los *0.26* segundos, cuando al poner poca desviación estándar se obtenían *0.03* segundos). Esto podría deverse a múltiples factores, pero al tener un tamaño de kernel calculado de forma automática, el hacerse con mayor desviación estándar podría indicar que se realiza el filtro con un tamaño de kernel mayor, lo que afectaría al tiempo al tener que consultar más vecinos por cada pixel.\n",
    "\n",
    "- **Median**: Se calcula la mediana, que es una operación rápida, por lo que el tiempo dedicado es pequeño. Al igual que en los anteriores cuanto más grande el tamaño del kernel más tarda en ejecutarse (de *0.01* a *0.05* segundos).\n",
    "\n",
    "- **Bilateral**: Es el que más tarda en ejecutarse. Tarda *2.2* segundos en los valores por defecto (15 y 15), pero si estos se disminuyen el tiempo así lo hace (con 7 tardan *0.5* segundos y con 1 tardan *0.01* segundos). El disminuir Sigma Color (a 3) manteniendo el Sigma Space(a 15) no baja en excesivo el tiempo (*2.08* segundos), pero al contrario baja a *0.07* segundos. \n",
    "\n",
    "  Esto tiene su explicación en que, como se explicó en la memoria, Sigma Space indica qué píxeles se consideran lo suficientemente cercanos para tenerse en cuenta (de forma que cuanto más cercano más peso tiene), por lo que su disminución implica tener en cuenta menos píxeles. Sin embargo, Sigma Color indica los colores, por lo que se tendrán que seguir revisando todos los píxeles para ver su color.\n",
    "\n",
    "- **Minimo** y **Máximo**: Desde *0.03* segundos con tamaño de kernel pequeño hasta *0.1* segundos con tamaño de kernel grande. \n",
    "\n",
    "  Al igual que los anteriores, el tener un tamaño de ROI pequeño hace que el tiempo disminuya (*0.1* tamaño de kernel y ROI grande, *0.004* tamaño de ROI pequeño y de kernel grande).\n",
    "\n",
    "- **Transformación de valor**: Al sumar un valor constante a cada pixel, el valor no influye en el tiempo. El tamaño del ROI influye, pero menos que otros debido a que no se deben realizar muchas operaciones para calcular una suma. Por ello, la diferencia va de *0.001* segundos hasta *0.004* segundos\n",
    "\n",
    "- **Ecualizador de histograma**: Dependiendo del tamaño del ROI, entre *0.001* y *0.004* segundos.\n",
    "\n",
    "- **CLAHE**: Es más pesado que el ecualizador de histogramas, ya que realiza histogramas locales en vez de uno global, por lo que tarda más (entre *0.001* y *0.01* segundos).\n",
    "\n",
    "- **Opening**: Dependiendo del tamaño del ROI, entre *0.001* y *0.003* segundos. No se realiza muchos cálculos, por lo que tiene sentido su tardanza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128d6f4-8503-4019-8a6a-20ef797ec7e8",
   "metadata": {},
   "source": [
    "### Observaciones globales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837390b7-426c-49d5-af27-b546f4b4a813",
   "metadata": {},
   "source": [
    "- Cuanto más grande es el kernel, más tarda el filtro en ejecutarse\n",
    "- Cuanto más grande es el ROI, más tarda el filtro en ejecutarse\n",
    "- Cuando más acciones debe realizar el filtro, más diferencia hay entre los casos rápidos (tamaño de ROI o/y kernel pequeño) y los casos lentos (tamaño del ROI o/y kernel grande)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4112f-4820-402c-b11e-df665ceffe4a",
   "metadata": {},
   "source": [
    "## Bibliografía usada\n",
    "\n",
    "[Filtros usados de OpenCV](https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html)\n",
    "\n",
    "[Más documentación de OpenCV](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9)\n",
    "\n",
    "[Ecualizador de histogramas de OpenCV](https://docs.opencv.org/3.4/d4/d1b/tutorial_histogram_equalization.html)\n",
    "\n",
    "[Preguntas a ChatGPT para aclarar conceptos](https://chat.openai.com)\n",
    "\n",
    "[CLAHE](https://en.wikipedia.org/wiki/Adaptive_histogram_equalization)\n",
    "\n",
    "[Función morphologyEx()](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga67493776e3ad1a3df63883829375201f)\n",
    "\n",
    "[Función erode()](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#gaeb1e0c1033e3f6b891a25d0511362aeb)\n",
    "\n",
    "[Función dilate()](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga4ff0f3318642c4f469d0e11f242f3b6c)\n",
    "\n",
    "[Como añadir trackbars a una ventana con OpenCV](https://docs.opencv.org/3.4/da/d6a/tutorial_trackbar.html)\n",
    "\n",
    "[Acceder a las subclases de una clase en Python](https://stackoverflow.com/questions/3862310/how-to-find-all-the-subclasses-of-a-class-given-its-name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3139d-b6f6-429b-a061-875c15299b02",
   "metadata": {},
   "source": [
    "# CLASIFICADOR y SIFT\n",
    "\n",
    "Como SIFT se trata de una ampliación de CLASIFICADOR, se ha decidido incluir ambos en un mismo notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a32ad-f145-46c5-a5df-246fb4279e52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enunciados\n",
    "\n",
    "**Enunciado de CLASIFICADOR:**\n",
    "\n",
    "Prepara una aplicación sencilla de reconocimiento de imágenes. Debe admitir (al menos) dos argumentos:\n",
    "\n",
    "- `--models=<directorio>`, la carpeta donde hemos guardado un conjunto de imágenes de objetos o escenas que queremos reconocer.\n",
    "\n",
    "- `--method=<nombre>`, el nombre de un método de comparación.\n",
    "\n",
    "Cada fotograma de entrada se compara con los modelos utilizando el método seleccionado y se muestra información sobre el resultado (el modelo más parecido o probable, las distancias a los diferentes modelos, alguna medida de confianza, etc.). Implementa inicialmente un método basado en el \"embedding\" obtenido por [mediapipe](https://developers.google.com/mediapipe/solutions/vision/image_embedder) (`code/DL/embbeder`).\n",
    "\n",
    "**Enunciado de SIFT:**\n",
    "\n",
    "Añade al ejercicio CLASIFICADOR un método basado en el número de coincidencias de `keypoints` SIFT. Utilízalo para reconocer objetos con bastante textura (p. ej. carátulas de CD, portadas de libros, cuadros de pintores, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2670922c-090d-4499-ab4b-0a07eb1f5b8d",
   "metadata": {},
   "source": [
    "## Ficheros\n",
    "\n",
    "- Carpeta `code`: Aquí se encuentran los ficheros de código\n",
    "- Carpeta `images`: Aquí se encuentran imágenes de prueba para el clasificador.\n",
    "- Carpeta `imports`: Aquí se encuentran los ficheros importantes para el funcionamiento del programa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2628cd0-af5f-4530-bdad-f090c1bcfb10",
   "metadata": {},
   "source": [
    "## Código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf61c75-1925-424e-b96e-2ec6a6007c2c",
   "metadata": {},
   "source": [
    "### clasificadorConstructor.py:\n",
    "\n",
    "En este fichero se encuentra la clase `Clasificador` cuyas subclases son los distintos métodos de clasificación que se pueden aplicar. \n",
    "\n",
    "Se crea un clasificador por cada imagen de ``models``, debido a que esto permite que cada instancia de subclase guarde transformaciones a la imagen en la inicialización de la instancia, y se haga una sola vez, en lugar de una vez por cada frame.\n",
    "\n",
    "De esta forma, en el método de inicialización (`\\_\\_init\\_\\_`), se pasa como parámetro el path a la imagen (\"imgPath\") y  el nombre de la imagen (\"nombreImg\"). Se guarda el nombre de la imagen, de forma que se pueda llamar al método `getNombreImg()` cada vez que este se necesite. \n",
    "\n",
    "Cada subclase tiene un método estático llamado \\textit{getMethod()} que devuelve el nombre del clasificador. Este es el nombre con el que se llama a ese clasificador concreto (`--method`) en los parámetros de entrada del programa. \n",
    "\n",
    "El método de clase `changeFrame` recibe como parámetro el frame actual. De esta forma, este método permite realizar las transformaciones necesarias al frame y guardarlas en variables de clase.\n",
    "\n",
    "Así, en lugar de hacer las transformaciones una vez por cada instancia, se hacen las transformaciones una vez por frame.\n",
    "\n",
    "El método llamado `similarity` devuelve la similaridad del frame actual (el que se guardó la última vez que se llamó a `changeFrame()`), con la imagen de la instancia. También devuelve el frame modificado según se quiera. Por ejemplo, en el método `skimageHog` se dibuja un rectángulo en el frame en la posición con mayor similaridad.\n",
    "\n",
    "### clasificadorColeccion.py:\n",
    "\n",
    "Contiene las subclases de la clase Clasificador. Cada subclase implementa \\textit{similarity}, \\textit{changeFrame}, y el constructor de la subclase. A su vez, cada una implementa el método `getMethod()` en el que devuelven el nombre del método.\n",
    "\n",
    "- **embedder**\n",
    "\n",
    "  Se inicializa la clase con el método `\\_\\_init\\_\\_`. Este método llama a un método de clase (`inicializoClase`) que crea un objeto `ImageEmbedder` con el modelo almacenado en `./embedder.tflite`. También se utiliza la imagen del clasificador para guardar su embedding (calculado con el modelo).\n",
    "\n",
    "  Es importante indicar que sólo se llamará a `inicializoClase` una vez (y no en todas las instancias), de forma que se comparte el modelo entre todas las instancias de esta subclase.\n",
    "\n",
    "  El método `changeFrame` guarda el frame pasado como parámetro y su embedding (calculado con el modelo).\n",
    "\n",
    "  En el método `similarity`, se utiliza un método de mediapipe que calcular la similaridad entre los embeddings del frame y de la imagen.\n",
    "\n",
    "  El frame se modifica mostrando la imagen y la similaridad obtenida, además del frame.\n",
    "\n",
    "- **skimageHog**\n",
    "\n",
    "  Se inicializa la clase con el método `\\_\\_init\\_\\_`, donde se guarda el nombre de la imagen de la clase, el histograma de orientaciones del gradiente (HOG) de esta imagen, la altura y anchura del HOG y el HOG aplanado (con el uso del método `flatten()`). \n",
    "\n",
    "  Dado que estas operaciones son muy lentas, guardar el HOG, y este aplanado, ayuda a no tener que calcularlo por cada frame, y por lo tanto que la aplicación resulte más rápida.\n",
    "\n",
    "  En el método `changeFrame` se guarda el HOG del frame, su altura y su anchura. No se guarda este aplanado porque se debe calcular en similarity, cómo se verá a continuación.\n",
    "\n",
    "  En el método `similarity` se calcula la distancia entre los HOGs de la imagen de la instancia y el frame almacenado.\n",
    "\n",
    "  La distancia entre HOGs de dos imágenes se calcula entre HOGs de igual tamaño, por lo que se calcula esta distancia entre el HOG de la imagen y cada una de las posibles partes del HOG del frame que tengan el mismo tamaño que el de la imagen. \n",
    "\n",
    "  Esto se hace mediante dos bucles que van desde el principio del HOG del frame (0,0) hasta la diferencia de altura o anchura de los HOGs (si se pasa de la diferencia, no se puede comparar con la imagen porque faltarían valores a la derecha/debajo del HOG del frame).\n",
    "\n",
    "  Esta distancia se calcula entre HOGs aplanados, de forma que resulte más cómoda su comparación. Una vez aplanado, bastaría con ir por cada bloque y sumar la diferencia de cada par de bloques del HOG, entre el número de bloques totales. Esta diferencia se calcula mirando la diferencia de la magnitud de los vectores de cada orientación.\n",
    "\n",
    "  De estas distancias, se busca la más pequeña, y se devuelve 1 - la distancia, de forma que cuanto más grande sea el valor, más parecidas son las imágenes.\n",
    "\n",
    "  Por esto el frame no se guarda aplanado, ya que se debe aplanar cada parte del HOG del frame para compararla con la imagen. Por ello, no sirve el HOG entero aplanado y se debe aplanar cada vez.\n",
    "\n",
    "  El frame en `similarity` se modifica, dibujando un rectángulo en la posición de la imagen con mayor similaridad. Se devuelve el frame modificado.\n",
    "\n",
    "  A su vez, se dibuja la distancia entre el frame y la imagen, y la imagen de la instancia en la esquina superior izquierda del frame.\n",
    "\n",
    "  Se devuelve este frame modificado, y 1 menos la distancia menor, de forma que cuanto más grande sea, más similares sean la imagen y la parte del frame seleccionada.\n",
    "\n",
    "- **sift**\n",
    "\n",
    "  Se inicializa la clase con el método `\\_\\_init\\_\\_`, que guarda el atributo \"bf\" que sirve para comparar los keypoints de dos imágenes y dar las dos mejores coincidencias para cada punto, y el atributo \"sift\" que sirve para detectar los keypoints de una imagen. Estos atributos se guardan en atributos de clase, para que todas las distintas instancias de la subclase las compartan.\n",
    "\n",
    "  También se guardan los keypoints y los descriptores de estos de la imagen cuyo nombre se pasa como parámetro, de forma que no se tiene que calcular por cada frame.\n",
    "\n",
    "  En el método `changeFrame` se utiliza \"sift\" para obtener los keypoints del frame pasado como parámetro, y sus descriptores.\n",
    "\n",
    "  En el método `similarity`, se obtienen los dos mejores matches de cada keypoint con \"bf\". Estos matches se calculan con los keypoints (sus descriptores) de la imagen de la instancia con el frame almacenado.\n",
    "\n",
    "  Posteriormente, se hace el test de ratio para quedarse solo con los mejores matches.\n",
    "\n",
    "  El test de ratio se basa en quedarse con la mejor coincidencia de keypoints solo si hay mucha diferencia entre el parecido de la mejor con el keypoint, y el parecido de la segunda mejor con el keypoint. De esta forma, se evita escoger coincidencias erróneas.\n",
    "\n",
    "  Después, se dibujan los matches en el frame y se escribe el número de keypoints que coincide. Se devuelve el frame modificado y la similaridad. \n",
    "\n",
    "  Para calcular la similaridad no se pueden devolver los keypoints que coinciden a secas, debido a que, si una imagen tuviese significativamente más keypoints que otra (por ejemplo 100 vs 10), es probable que termine teniendo más matches con el frame aunque el frame (o un trozo de este) se parezca más a la imagen con pocos keypoints. Para evitar esto, se divide el número de keypoints que coinciden entre el número de keypoints que tiene la imagen; de forma que se mira el porcentaje de keypoints de la imagen que se han encontrado en el frame.\n",
    "\n",
    "### clasificador.py:\n",
    "Es la clase principal del programa, es la que se ejecuta para que funcione el clasificador.\n",
    "\n",
    "En primer lugar, se recogen los parámetros de entrada del programa (directorio de modelos y método de comparación). Se comprueba que se han pasado los dos parámetros necesarios, que no se han escrito parámetros no conocidos, y que el directorio de modelos existe.\n",
    "\n",
    "Se obtiene el clasificador (de la clase Clasificador). Para ello, se recorren las subclases y se queda con la subclase cuyo atributo método coincide con el método pasado como parámetro. Si este no existe, se muestra un mensaje indicándolo y se cierra el programa.\n",
    "\n",
    "Cuando se tiene la subclase, se crea una instancia de esta por cada imagen del directorio de modelos.\n",
    "\n",
    "Se empieza a capturar el vídeo. Por cada frame que se captura, se llama al método `changeFrame` de la subclase escogida. Se llama al método `similarity` con todas las instancias de la subclase y se almacena el frame (modificado) devuelto por la instancia que devuelva la mayor similaridad. Por último, se muestra por pantalla el frame modificado.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5129e7-fb39-4ae1-9d27-9bdf8c686908",
   "metadata": {},
   "source": [
    "## Conceptos teóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bf984-88f8-4d12-af03-3d1c9a5ffe32",
   "metadata": {},
   "source": [
    "**Gradiente**:\n",
    "\n",
    "El gradiente es un vector que indica hacia donde aumenta la luz. El gradiente de una imagen está compuesto por vectores que indican donde aumenta la luz por toda la imagen.\n",
    "\n",
    "Cuando se trabaja con una imagen en blanco y negro, los cambios entre zonas blancas y zonas negras suelen ser zonas de bordes. Por ello, los cambios de gradiente suelen indicar la presencia de un borde. Al tratar con objetos, el fijarse en sus bordes realmente es fijarse en la estructura general de este.\n",
    "\n",
    "Para utilizar el gradiente para clasificar objetos, en primer lugar se debe de suavizar la imagen para eliminar el ruido de la imagen que se toma del objeto y fijarse en el nivel justo de detalle, de forma que no se fije en los detalles pequeños sino en la estructura general de este.\n",
    "\n",
    "Tal y como se ha dicho, el gradiente ayuda a reconocer objetos. Esto ocurre si el objeto es rígido, o tiene deformaciones pequeñas, de forma que una vez obtenido su gradiente este no cambia (o no lo hace de forma significativa). Para comparar dos fotografías y conocer si son el mismo objeto, en lugar comparar los gradientes, se utilizan los histogramas de orientaciones del gradiente.\n",
    "\n",
    "**HOG**:\n",
    "\n",
    "El HOG, también llamado el histograma de orientaciones del gradiente, es un conjunto de histogramas locales sobre las orientaciones discretizadas del gradiente.\n",
    "\n",
    "Los vectores del gradiente, en lugar de representarlos con coordenadas x e y, se pueden representar de una forma polar con la magnitud del vector y su orientación. Esta forma de representarlos es muy útil, permitiendo la discretización de las orientaciones, dividiendolas en un cierto número (por ejemplo 16). En el caso de skimageHog, hay 8 orientaciones distintas.\n",
    "\n",
    "Esta discretización de las orientaciones permite identificar objetos aunque hayan pequeños cambios.\n",
    "\n",
    "Además, se crean histogramas locales de los vectores, de forma que se agrupan los vectores de píxeles cercanos y por cada agrupación se guardan las magnitudes totales de los vectores hacia cada orientación posible. Esta magnitud total en cada orientación se puede calcular de distintas formas. \n",
    "\n",
    "Una forma es sumar el número de vectores con esa orientación, pero esta forma no tiene en cuenta las diferencias de vectores, ya que no es lo mismo un vector que va de blanco a negro que un vector que va de un gris más claro a un gris más oscuro.\n",
    "\n",
    "Una buena forma de calcular la magnitud total en cada orientación es sumando la magnitud de los vectores (locales) que están en esa orientación.\n",
    "\n",
    "Una vez se tienen estos histogramas locales, se suelen normalizar. Esta normalización puede ser con sólo el histograma local. Sin embargo, es mejor normalizar cada histograma local teniendo en cuenta los histogramas locales cercanos a este.\n",
    "\n",
    "De esta forma, se termina consiguiendo un HOG (conjunto de histogramas locales) que permite identificar objetos aunque hayan pequeños cambios en estos. Es importante recordar que esto se debe realizar sobre las fotografías en blanco y negro.\n",
    "\n",
    "**SIFT**:\n",
    "\n",
    "Este método calcula los ``keypoints'' de una imagen. Esto se refiere a puntos en una imagen que se diferencian de su entorno y, por lo tanto, el verlos en otra imagen hace que se puedan reconocer.\n",
    "\n",
    "Por ejemplo, el borde de una mesa no es un buen punto debido a que este borde es igual al borde de la mesa un poco más a la izquierda. Sin embargo, las esquinas de la mesa sí son puntos clave, ya que no se parecen a su entorno. Por ello, es necesario escoger los puntos clave de la imagen. \n",
    "\n",
    "Esta búsqueda de puntos clave (keypoints) se calcula en el código del ejercicio con la función de OpenCV \"detectAndCompute\" aplicada a la imagen correspondiente, con el detector de puntos clave creado con la función \"SIFT\\_create()\" de OpenCV.\n",
    "\n",
    "A su vez, la función \"knnMatch\" aplicada a los descriptores de los puntos clave de dos imágenes calcula las mejores coincidencias de puntos clave entre las imágenes. Esto se hace con un comparador de puntos clave de OpenCV que se crea con la función \"FMatcher\".\n",
    "\n",
    "Esto ayuda a comparar imágenes, de modo que cuantas más coincidencias de puntos clave obtengan, más se parecerán. Para ello, se debe hacer una limpieza de puntos, de manera que los puntos con cierta coincidencia pero con una coincidencia parecida con la segunda mejor coincidencia no se debe tener en cuenta, pero los que tienen una coincidencia muy distinta a la segunda mejor coincidencia se tienen en cuenta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf6f0a-e6f8-4ba4-9215-375b244fa65f",
   "metadata": {},
   "source": [
    "## Ejecución del código: Ejemplo de funcionamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7a4b1-4c0d-48f8-82b9-dc740bd2579a",
   "metadata": {},
   "source": [
    "Para ejecutar el código de CLASIFICADOR, se debe de ejecutar el código de `clasificador.py` desde la carpeta `CLASIFICADOR + SIFT` en el entorno de anaconda prompt explicado al inicio de la asignatura. Se deben de pasar como parámetros la carpeta en la que se encuentran las imágenes y el nombre del método. Si por ejemplo, se quiere realizar el método *Embedder* con la carpeta de fotografías que se encuentra ejecutando *cd ../images/ImagesEmbedder*, se debe de ejecutar el siguiente comando:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24777409-cf05-4ca5-a15b-21603bf8e5a8",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">clasificador.py --method=embedder --models=../images/ImagesEmbedder</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6b15e-ce57-4039-8e4e-1a10a40abd47",
   "metadata": {},
   "source": [
    "Es relevante mencionar que se muestra la imagen a la que más se parece el frame actual arriba a la izquierda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec96e3e-051d-42af-a576-884ec88e1083",
   "metadata": {},
   "source": [
    "- **Embedder**:\n",
    "\n",
    "  Se van a presentar algunas imágenes del funcionamiento del clasificador Embedder. Si se utiliza una carpeta con imágenes  (models) de distintas zonas de una habitación (incluso personas) las identifica de zona correcta, aunque la imagen tenga distinta luminancia, resolución, o tamaño que los frames tomados con la cámara actual:\n",
    "\n",
    "  <table style=\"width: 70%\"><tr>\n",
    "  <td> <img src=\"Clasificador%20+%20SIFT/img/embedder-cara.png\"/> </td>\n",
    "  <td> <img src=\"Clasificador%20+%20SIFT/img/embedder-otra-camara-luz.png\"/> </td>\n",
    "  <td> <img src=\"Clasificador%20+%20SIFT/img/embedder-pared.png\"/> </td>\n",
    "  </tr></table>\n",
    "\n",
    "  A su vez, funciona con cuadros, aunque estos se encuentren movidos o con colores algo alterados:\n",
    "\n",
    "  <table style=\"width: 70%\"><tr>\n",
    "  <td> <img src=\"Clasificador%20+%20SIFT/img/cuadro-2.png\"/> </td>\n",
    "  <td> <img src=\"Clasificador%20+%20SIFT/img/cuadro-1.png\"/> </td>\n",
    "  <td> <img src=\"Clasificador%20+%20SIFT/img/cuadros-error.png\"/> </td>\n",
    "  </tr></table>\n",
    "\n",
    "  Como se ha podido observar, el clasificador falla en caso de que la fotografía se encuentre girada y más pequeña.\n",
    "\n",
    "  Por lo tanto, para observar habitaciones funciona de forma correcta, pero con objetos (como cuadros) son preferibles otros clasificadores, a menos que estos objetos se muestren en la misma posición, distancia y ángulo que en la fotografía.\n",
    "\n",
    "  Por último indicar que con objetos simples funciona de forma correcta a pesar de la diferencia de color entre la imagen y el frame, como se puede observar a continuación:\n",
    "\n",
    "  <video src=\"Clasificador%20+%20SIFT/img/objetos-simples-embedder.mp4\" controls='play' style=\"width:40%\">\n",
    "  </video>\n",
    "\n",
    "- **skimageHog**:\n",
    "\n",
    "  En este filtro, hay que tener cuidado con qué objeto se escoge, ya que si se escoge un objeto muy simple, podría confundirse con otras cosas. Por ejemplo, en el siguiente vídeo, en vez de identificar las gafas todo el tiempo, en ocasiones se confunde con el fondo de la imagen al pensar que es un ratón:\n",
    "\n",
    "  <video src=\"Clasificador%20+%20SIFT/img/confunde-gafas-raton.mp4\" controls='play' style=\"width:40%\">\n",
    "  </video>\n",
    "\n",
    "- **sift**:\n",
    "\n",
    "  El método SIFT funciona de forma correcta en cuadros y objetos complejos, pero hay que tener cuidado con los objetos simples, ya que, al tener pocos keypoints, son dificilmente identificables. Un ejemplo es el siguiente vídeo:\n",
    "\n",
    "  <video src=\"Clasificador%20+%20SIFT/img/objetos-simples-sift.mp4\" controls='play' style=\"width:40%\">\n",
    "  </video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b0e97-d9d2-48af-9b50-8fca9c751738",
   "metadata": {},
   "source": [
    "## Tardanza del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e7d02-7d57-4a25-b69e-4fa96fc61bfd",
   "metadata": {},
   "source": [
    "Para ver cuanto tarda en ejecutar el código se va a añadir las siguientes líneas de código al inicio del programa (antes de mostrar frames), y cuando se captura y cuando se muestra el frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6cf52a-4acd-4580-8693-beef9a0a39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# ...\n",
    "inicio = time.time()\n",
    "\n",
    "# Código que se hace cada frame\n",
    "\n",
    "fin = time.time()\n",
    "print(fin-inicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b095b81-f1e0-4e98-be6b-7aed141943a8",
   "metadata": {},
   "source": [
    "Como se puede observar, este código tarda 0 segundos porque no hace nada en medio, pero al añadirlo a `clasificador.py`, el tiempo aumenta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981f32a-92fd-4471-8f1a-6225546572d6",
   "metadata": {},
   "source": [
    "Se obtiene:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745557b-00fd-48a3-9023-20b4e6fc46ae",
   "metadata": {},
   "source": [
    "**Inicio** (con 1 carpeta que contiene 4 imágenes de distintos cuadros):\n",
    "- **Embedder**: *0.92* segundos\n",
    "- **skimageHog**: *3.4* segundos\n",
    "- **sift**: *5.8* segundos\n",
    "\n",
    "**Cada frame** (con una carpeta que 4 imágenes de distintas zonas de una habitación):\n",
    "- **Embedder**: Entre *0.01* y *0.02* segundos\n",
    "- **skimageHog**: Entre *0.07* y *0.12* segundos\n",
    "- **sift**: Entre *0.15* y *0.26* segundos\n",
    "\n",
    "Se probó moviendo mucho la cámara, de ahí la diferencia de segundos en cada frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93398839-8279-4b92-9189-8d3e7c1d7c1b",
   "metadata": {},
   "source": [
    "Como se ha podido obervar, *Embedder* es el método que menos tarda en ejecutarse, seguido por *skimageHog*, y por último *sift*.\n",
    "\n",
    "Se tarda en iniciar debido a que se realizan tareas al inicio con las imágenes de forma que no se tengan que realizar por cada frame (y de este modo reducir el tiempo que tarda en mostrar cada frame y que vaya más fluido)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b712e62-8fee-4fc1-8eff-6f6541294a69",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "[MediaPipe Image Embedder](https://developers.google.com/mediapipe/solutions/vision/image_embedder/python)\n",
    "\n",
    "[Comprobar la existencia de un fichero](https://www.geeksforgeeks.org/python-os-path-exists-method/)\n",
    "\n",
    "[Comprobar la existencia de un directorio](https://www.geeksforgeeks.org/python-os-path-isdir-method/)\n",
    "\n",
    "[Recorrer archivos de un directorio](https://www.codigopiton.com/como-listar-archivos-de-carpeta-en-python/)\n",
    "\n",
    "[Material de la asignatura (código y notebooks)](https://github.com/albertoruiz/umucv/blob/master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7e9e1-6a0f-4d5f-90a7-be657a55a1db",
   "metadata": {},
   "source": [
    "# RECTIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27924a-0f12-431e-92f6-ceff5ccf6899",
   "metadata": {},
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0a110-4684-4e1d-a80e-3212cbb49c68",
   "metadata": {},
   "source": [
    " Rectifica la imagen de un plano para medir distancias (tomando manualmente referencias conocidas). Por ejemplo, mide la distancia entre las monedas en `coins.png` o la distancia a la que se realiza el disparo en `gol-eder.png`. Las coordenadas reales de los puntos de referencia y sus posiciones en la imagen deben pasarse como parámetro en un archivo de texto. Aunque puedes mostrar la imagen rectificada para comprobar las operaciones, debes marcar los puntos y mostrar el resultado sobre la imagen original. Verifica los resultados con **imágenes originales** tomadas por ti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc6e76-1893-4a7f-81c2-daecbf178ce5",
   "metadata": {},
   "source": [
    "## Explicación del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aa988f-920e-4c9a-a7ad-0c59d685080f",
   "metadata": {},
   "source": [
    "### Funciones de otros utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a2043-82a8-451a-80d1-e8401a725487",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Se utilizan funciones proporcionadas en los [apuntes de la asignatura](https://github.com/albertoruiz/umucv/blob/96a0e8bcc9e95151c309ac48f743e4fd8a90a77c/notebooks/transf2D.ipynb), de modo que se tiene una función (*htrans*) que aplica una transformación homogénea h a un conjunto de puntos (\"tradicionales\", no homogéneos).\n",
    "\n",
    "A su vez, se utiliza una función de OpenCV llamada *findHomography*, que dado dos conjuntos de coordenadas (que se refieren a los mismos puntos), devuelve la homografía (transformación que se aplica a las coordenadas de un conjunto para obtener el otro)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2432e77-e84e-4654-a859-7ef81a0890ed",
   "metadata": {},
   "source": [
    "### Código implementado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6ff34-ad14-4ae8-9a4e-868ac9c60463",
   "metadata": {},
   "source": [
    "Se debe pasar la ruta de dos archivos como parámetros del script:\n",
    "\n",
    "- La imagen sobre la que se medirán distancias\n",
    "- El archivo con las coordenadas de los puntos de referencia en la imagen y en la realidad\n",
    "\n",
    "A partir de estos dos archivos, se obtiene la imagen y las coordenadas.\n",
    "\n",
    "Una vez leídas las coordenadas, se calcula a partir de ellas la homografía (usando *findHomography*). Con la homografía calculada, ya se pueden medir distancias:\n",
    "\n",
    "- Se muestra la imagen para que se seleccionen los puntos\n",
    "- El usuario selecciona dos puntos en la imagen clicando sobre ellos (se guarda el punto y se pinta un círculo para indicar dónde se ha seleccionado)\n",
    "- Una vez seleccionados dos puntos, se calculan las coordendas que tendrían en la realidad estos puntos (utilizando *htrans* con los puntos seleccionados y la homografía ya calculada)\n",
    "- Con las coordenadas reales, se calcula la distancia entre los dos puntos, obteniendo la distancia real entre los puntos. Una vez obtenida, sólo queda dibujar la distancia y esperar a que se vuelvan a seleccionar dos puntos para empezar de nuevo\n",
    "\n",
    "El último aspecto a resaltar es que, cuando se espera a seleccionar de nuevo dos puntos, la imagen sigue mostrando la distancia entre los puntos seleccionados anteriormente. De esta forma, cuando se hace clic por primera vez, se vuelve a obtener la imagen sin dibujar antes de pintar un circulo sobre el primer punto seleccionado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969eea82-6a09-436a-ab31-06263ada6099",
   "metadata": {},
   "source": [
    "### Formato del archivo de coordenadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806e6fc-cf99-4023-bc54-b36211fed261",
   "metadata": {},
   "source": [
    "El archivo de coordenadas debe tener el siguiente formato:\n",
    "\n",
    "```\n",
    "x1 , y1 - X1 , Y1\n",
    "x2 , y2 - X2 , Y2\n",
    "...\n",
    "```\n",
    "\n",
    "De forma que cada línea contiene las coordenadas de un punto de referencia. Las coordenadas x e y se refieren a la posición del punto en la imagen, mientras que las coordenadas X e Y se refieren a la posición real del punto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a47804-b546-4569-8e8f-0c3b5ac0662e",
   "metadata": {},
   "source": [
    "## Ejecución del código: Ejemplos de funcionamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b604553-ed32-4e91-9330-b5e325e8df9f",
   "metadata": {},
   "source": [
    "Se va a mostrar un ejemplo de funcionamiento con *coins.png*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc02b7-2d40-4b22-80ab-f47e3e94f497",
   "metadata": {},
   "source": [
    "1. Se ejecuta el programa (*RECTIF.py --imagen coins.png --coordenadas coins.txt*)\n",
    "   - coins.png es la imagen sobre la que se medirán distancias\n",
    "   - coins.txt es el archivo con las coordenadas de los puntos de referencia.\n",
    "\n",
    "2. Se muestra la imagen para seleccionar dos puntos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115acef0-a3a6-412a-a5ae-292ff190e997",
   "metadata": {},
   "source": [
    "<img src=\"RECTIF/img/imagen-selecciona.png\" style=\"width:35%\"> </img>\n",
    "<br><em>Paso 2 (Imagen mostrada para seleccionar los puntos)</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f1146-0bd4-44ca-bc56-5693c568ea5c",
   "metadata": {},
   "source": [
    "3. Se seleccionan los puntos y se muestra la distancia real entre los puntos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccade991-4532-4ae3-ace0-15d3a13d9d97",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"RECTIF/img/seleccionado-1-punto.png\" style=\"width:90%\">\n",
    "        </td>\n",
    "        <td> <img src=\"RECTIF/img/seleccionados-2-puntos.png\" style=\"width:90%\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> <em>Seleccionado el primer punto</em> </td>\n",
    "        <td> <em>Seleccionado el segundo punto</em> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb90a55-2b39-4228-9c4a-8ef072d86c82",
   "metadata": {},
   "source": [
    "Se puede observar como se muestra la distancia entre el 0 y el 5 de la regla, lo que da 5 centímetros (pone 50 porque las coordenadas estaban en milímetros).\n",
    "\n",
    "Es dificil atinar al punto exacto, por lo que se suelen obtener medidas aproximadas, como en el ejemplo que indica 50.95 en vez de 50.0 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fd82b-18ce-45eb-848b-8b3b68581ae8",
   "metadata": {},
   "source": [
    "## Verificación con imagen tomada por mi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223eafc0-782d-4ba7-812d-ca22884b29cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "La imagen es la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0395b-e3cd-4295-945e-aa2fb2ce9e2e",
   "metadata": {},
   "source": [
    "<img src=\"RECTIF/foto-mesa.jpeg\" style=\"width:20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846565c-6070-4f47-a32e-2ebf8bc35d09",
   "metadata": {},
   "source": [
    "Para obtener los puntos, se han seleccionado las esquinas del cuadrado rojo (90mm cada lado). Se han obtenido las coordenadas en la imagen con *puntos.py* (*puntos.py --imagen foto-mesa.jpeg*, y cuando se seleccionan los 4 puntos se cierra el programa y se indica por terminal los puntos).\n",
    "\n",
    "Después de crear el archivo con las coordenadas (*foto-mesa.txt*), se procede a comprobar que si se seleccionan 2 puntos de la imagen (por ejemplo, de la regla), las medidas son correctas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7bd4f-5d9c-4c5e-9cfc-b9bd33e2437c",
   "metadata": {},
   "source": [
    "<img src=\"RECTIF/img/foto-3-cm.png\" style=\"width:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f952008-a794-480b-98c6-ae5e07f64db2",
   "metadata": {},
   "source": [
    "Como se puede observar, al seleccionar 3 cm en la regla, se indica que son 29 milímetros (se dijo antes que las coordenadas escritas estaban en milímetros, por lo que las medidas también lo estarán).\n",
    "\n",
    "Indica 29.31 y no 30.0 porque es muy dificil atinar con los puntos en el lugar exacto, pero queda comprobado que las medidas funcionan de forma correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31acb45d-5131-47b7-8786-b1b6d2b03d1b",
   "metadata": {},
   "source": [
    "## Teoría utilizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3812509-464a-477f-aecd-e67cea6a657c",
   "metadata": {},
   "source": [
    "Para realizar el programa, ha sido necesario entender las coordenadas homogéneas, cuya explicación se encuentra en [coordhomog.ipynb](https://github.com/albertoruiz/umucv/blob/96a0e8bcc9e95151c309ac48f743e4fd8a90a77c/notebooks/coordhomog.ipynb), y el funcionamiento de las homografías, cuya explicación se encuentra en [transf2D.ipynb](https://github.com/albertoruiz/umucv/blob/96a0e8bcc9e95151c309ac48f743e4fd8a90a77c/notebooks/transf2D.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb7cec-ee4d-4c37-95e9-9f1d3e64ea9d",
   "metadata": {},
   "source": [
    "## Bibliografía usada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398b03f-322a-45e2-8f47-45c3536fa44c",
   "metadata": {},
   "source": [
    "[Apuntes de la asignatura](https://github.com/albertoruiz/umucv)\n",
    "\n",
    "[función *open*](https://docs.python.org/3/library/functions.html#open)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40565eb-e400-4de3-9af5-c8c74a7f8ef0",
   "metadata": {},
   "source": [
    "# RA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04267b74-3529-4353-a48e-d5394f355fa0",
   "metadata": {},
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00af58-8374-4fae-bd86-fd55ef05f682",
   "metadata": {},
   "source": [
    "Crea un efecto de realidad aumentada en el que el usuario desplace objetos virtuales hacia posiciones marcadas con el ratón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876747b-8519-4ac2-82aa-14735aabf0bc",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b473b4-7b5c-4484-8d05-d051ec857778",
   "metadata": {},
   "source": [
    "En primer lugar, se van a explicar las variables importantes para el funcionamiento del código:\n",
    "\n",
    "1. **desplazamiento**: Contiene el punto 3D destino del objeto virtual \n",
    "2. **actual**: Contiene el punto 3D en el que se encuentra el objeto virtual\n",
    "3. **start** y **now**: start contiene la última vez que se mostró un frame y now contiene el tiempo actual. Con start y now se puede saber cuanto tiempo ha transcurrido desde la última vez que se mostró un frame (y por lo tanto, desde que se movió el objeto).\n",
    "4. **K_Matriz**: Matriz de calibración de la cámara. Si se usa otra habría que cambiarla.\n",
    "5. **marker**: Los puntos 3D de los que se compone el marcador.\n",
    "6. **objeto**: Los puntos 3D de los que se compone el objeto virtual\n",
    "7. **point** y **pulsado**: point contiene el pixel que se pulsa en la pantalla, y pulsado contiene True si se acaba de pulsar y aún no se ha calculado a dónde debe ir el objeto virtual (*desplazamiento*), y False si ya se ha calculado "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42a986-655e-4d43-8f59-ca83d33bb79a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hay otras variables, pero no son globales, sino que se crean en cada frame. Por ello, se prosigue con la explicación del código del bucle principal (se llama una vez por frame):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478847a8-dadf-483e-af5f-4da43ecedc57",
   "metadata": {},
   "source": [
    "1. Se guarda el tiempo actual en **now**.\n",
    "\n",
    "2. Se extraen los contornos del frame (extractContours de umucv) y se almacenan los que se pueden reducir a 6 vértices (el marcador tiene 6 vértices y queremos encontrar el marcador).\n",
    "\n",
    "    2.1. Si no ha encontrado ninguno, se muestra el frame y se pasa a otra iteración del bucle principal.\n",
    "\n",
    "3. Se usa la función *bestPose* explicada en [los apuntes de la asignatura](https://github.com/albertoruiz/umucv/blob/96a0e8bcc9e95151c309ac48f743e4fd8a90a77c/notebooks/camera.ipynb) para iterar por todos los contornos de 6 vértices y obtener el que más probable sea el marcador. \n",
    "\n",
    "    bestPose dado el modelo y el contorno, calcula la matriz de cámara que debería haber para que el marcador con esa matriz de cámara diera el contorno. Devuelve la matriz de cámara y el error de reproyección.\n",
    "    \n",
    "    De este modo, si se tiene mucho error, indica que probablemente no sea el marcador lo que se ha detectado. Por ello, nos quedamos con el contorno que menor error de reproyección tenga (y por lo tanto, el que más probablemente sea el marcador). Almacenamos la matriz de cámara de la mejor opción, los puntos que constituyen su contorno, y su error de reproyección.\n",
    "\n",
    "    3.1. Si el error no es menor a 4, se considera que no se ha encontrado el marcador en el frame, por lo que se muestra el frame y se pasa a otra iteración del bucle principal.\n",
    "\n",
    "4. Se dibuja el contorno del marcador encontrado en rojo.\n",
    "\n",
    "    4.1. Si no se había marcado ningún punto, se dibuja en verde el objeto virtual encima del marcador. Para ello, se obtienen los píxeles correspondientes a los puntos del objeto virtual aplicándole la matriz de cámara (htrans) al objeto en la posición actual. El objeto en la posición actual (que en el inicio es la posición en la que se encuentra el marcador) se obtiene de aplicar un desplazamiento al objeto (htrans con la función *desp* de la posición actual).\n",
    "\n",
    "5. Si *pulsado* es True, se debe calcular el desplazamiento al que debe ir el objeto virtual. Para ello, se elimina la tercera columna de la matriz de cámara (es la columna que se refiere al eje z, si se elimina entonces se obtiene una altura de 0), y se invierte. Una vez invertida, se aplica al punto marcado (*point*). De esto se obtiene la posición del punto en 3D (*desplazamiento*) ya que, como la matriz de cámara funciona para obtener de un punto en 3D al píxel en el frame, si se invierte se puede calcular del píxel en el frame al punto en 3D. *pulsado* se pone a falso para que no se vuelva a calcular.\n",
    "\n",
    "    Es importante que se calcule la posición, a partir del píxel donde debe ir el objeto, enseguida y después no se vuelva a calcular. Esto es así porque la posición destino se calcula a partir de la matriz de cámara, y por lo tanto si se mueve la posición del marcador (en la realidad y por lo tanto se ve en cada frame), entonces la matriz de cámara cambia.\n",
    "\n",
    "6. Si *desplazamiento* no es igual a *actual*, entonces se debe de mover la posición del objeto virtual para ir a *desplazamiento*. Para esto:\n",
    "\n",
    "    6.1. Se calcula el tiempo transcurrido desde el útimo frame (con *now* y *start*)\n",
    "    \n",
    "    6.2. El desplazamiento que necesita hacer el objeto virtual para llegar a *desplazamiento* es *desplazamiento* - *actual*.\n",
    "    \n",
    "    6.3. El vector de desplazamiento para llegar se divide entre la norma de este para obtener el vector con longitud 1. Después, se multiplica por el tiempo transcurrido entre 1.5. De este modo, se mueve poco a poco el objeto. Si se obtiene un desplazamiento mayor al que se necesita para llegar al destino, nos quedamos con el desplazamiento total para llegar ignorando lo hecho.\n",
    "    \n",
    "    6.4. Se actualiza la posición actual del objeto virtual, sumándole el desplazamiento a realizar (calculado en el punto anterior)\n",
    "    \n",
    "    6.5. Se dibuja el objeto virtual en verde en la posición actual. Para esto, se obtienen los píxeles correspondientes a los vértices del objeto (se aplica con htrans tanto la posición actual como la matriz de cámara).\n",
    "    \n",
    "\n",
    "7. Si *desplazamiento* es igual a *actual*, se dibuja el objeto en la posición actual. Para esto, se obtienen los píxeles correspondientes a los vértices del objeto (se aplica con htrans tanto la posición actual como la matriz de cámara).\n",
    "\n",
    "8. Se guarda el tiempo actual en start, de forma que se sepa el tiempo en el que se hizo el último frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320bb1f8-d168-42a0-b04e-59ecdf0ffb43",
   "metadata": {},
   "source": [
    "## Tardanza del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d8518-ce23-4b8b-85f5-d6eda4f93ebe",
   "metadata": {},
   "source": [
    "Para ver cuanto tarda en ejecutar el inicio del código (hasta que se muestra el primer frame), se va a añadir las siguientes líneas de código cuando empieza y cuando se captura el primer frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea06d9-d63c-4cfb-b102-463cd92459ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# ...\n",
    "inicio = time.time()\n",
    "\n",
    "# Código que se hace cada frame\n",
    "\n",
    "fin = time.time()\n",
    "print(fin-inicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65190f-a8b1-4b9e-929b-bd86e07d96c4",
   "metadata": {},
   "source": [
    "Como se puede observar, este código tarda 0 segundos porque no hace nada en medio, pero al añadirlo a `RA.py`, el tiempo aumenta, obteniendo 6.36 segundos de espera antes de obtener el primer frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11a908-a67c-4920-a05f-0c4d2f30e92e",
   "metadata": {},
   "source": [
    "Posteriormente, si se cuenta el tiempo entre frames (de la misma forma), se obtiene:\n",
    "\n",
    "- Una media de 0.02 segundos si se observa un marcador (varía normalmente entre 0.01 y 0.03, obteniendo en pocas ocasiones 0.04)\n",
    "- Una media de 0.003 segundos si no se observa ningun contorno en la cámara, y por lo tanto ningún marcador (tiene ocasiones en las que se obtienen muy pocos segundos como 0.001 o más de la media como 0.007)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c1581-1d09-4476-85e1-af7ea0943ff6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Aunque se debe esperar un tiempo apreciable antes de comenzar a ver la cámara, gracias al poco tiempo entre frames, una vez se inicia se observa de forma correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675681c5-fdcf-4f5c-8952-7e20b46b06d2",
   "metadata": {},
   "source": [
    "## Objetos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297eda9f-efb1-458f-bb58-dfbee06c2c6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Se puede elegir el objeto con **--objeto**, siendo por defecto una casa (no hace falta indicarlo), pero se puede cambiar por una pirámide (*--objeto \"piramide\"*) o un reloj de arena (*--objeto \"reloj-arena*). Se pueden observar a continuación los objetos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d7b1c-da4c-4c1a-933b-9d4c4f4e490d",
   "metadata": {},
   "source": [
    "Casa:\n",
    "\n",
    "<img src=\"RA/img/casa.png\" style=\"width:20%\"/> </td>\n",
    "\n",
    "Pirámide:\n",
    "\n",
    "<img src=\"RA/img/piramide.png\" style=\"width:20%\"/> </td>\n",
    "\n",
    "Reloj de arena:\n",
    "\n",
    "<img src=\"RA/img/reloj-arena.png\" style=\"width:20%\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56a79d-e966-4dad-ae4e-b788ad23a9f1",
   "metadata": {},
   "source": [
    "## Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f523d72-9f26-458b-b585-c50f7b854711",
   "metadata": {},
   "source": [
    "Si se quiere probar el código, pero no se tiene un marcador en físico, se puede probar con la imagen \"*marker.jpg*\" que se encuentra en esta carpeta, de forma que se ve esta imagen en lugar de la cámara.\n",
    "\n",
    "Si se quiere hacer, el comando es ***RA.py --dev \"./marker.jpg\" --loop***, pero también se puede ejecutar con los otros objetos, con ***RA.py --dev \"./marker.jpg\" --loop --objeto \"piramide\"*** o ***RA.py --dev \"./marker.jpg\" --loop --objeto \"reloj-arena\"***.\n",
    "\n",
    "Es importante indicar que se ha adaptado el código para la cámara que tenía en casa. Se ha utilizado el código de **calibrate** de los apuntes de la asignatura para obtener la matriz de calibración y almacenarla en la variable **K_Matriz**. Si se quiere usar otra cámara, sería necesario cambiar la matriz de calibración a la usada por esa cámara (cambiar el valor de *K_Matriz*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420db484-d4f2-40b5-8b65-0bcff1d5f772",
   "metadata": {},
   "source": [
    "## Teoría utilizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10b0fd-d33d-425c-983b-77521c2edf31",
   "metadata": {},
   "source": [
    "Para realizar el programa, ha sido necesario entender cómo funciona el modelo de cámara (matriz de calibración, matriz de cámara y cómo se transforman los puntos del espacio 3D en píxeles). Todo esto se ha comprendido con las explicaciones de los [apuntes de la asignatura](https://github.com/albertoruiz/umucv/blob/master/notebooks/camera.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a730f-6a8c-4229-9616-db79012b8592",
   "metadata": {},
   "source": [
    "# MAPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b656fe-d11d-412b-84bf-b8642fc57fba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enunciado\n",
    "\n",
    "Amplia el ejemplo `code/medidor.py` para convertir la distancia entre dos pixels marcados con el ratón en el ángulo que forman los rayos ópticos correspondientes, sabiendo el campo visual (FOV) de la cámara. Utiliza el script para encontrar mediante una construcción geométrica la posición aproximada en un mapa desde la que se ha tomado una imagen en la que se ven algunos puntos característicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56d059-faff-4a74-ab7c-6965ed067d6a",
   "metadata": {},
   "source": [
    "## FOV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430451cb-2f1f-4de1-86e0-e7ef441f4b96",
   "metadata": {},
   "source": [
    "En primer lugar, se va a calcular el FOV de la cámara dada su distancia focal. Para ello, se debe recordar la definición de campo visual y distancia focal. Se presenta un dibujo para que se comprenda mejor (en este caso, se ve el FOV horizontal):\n",
    "\n",
    "<img src=\"MAPA/img/semana2-triangulos.png\" style=\"width:40%\"/>\n",
    "\n",
    "Como se puede observar, la tangente de $\\frac{FOV}{2}$ es igual a $\\frac{w/2}{f}$ por la definición de tangente (en triángulos rectángulos, la tangente es la razón entre el cateto opuesto y el adyacente). De este modo, se obtiene la fórmula:\n",
    "\n",
    "$$tan\\left(\\frac{FOV_{horizontal}}{2}\\right) = \\frac{w/2}{f}$$\n",
    "\n",
    "Si se quiere calcular el FOV vertical, bastaría con cambiar el ancho de la imagen por el largo de esta. Serviría la fórmula anterior, siendo $w$ la altura en vez del ancho de la imagen.\n",
    "\n",
    "**Cálculos**\n",
    "\n",
    "Ya se tiene la distancia focal, que se calculó en el ejercicio HANDS, en el caso de mi cámara es 738 píxeles. Para despejar la fórmula es necesario indicar que la cámara toma fotografías de un ancho de 640 píxeles (w).\n",
    "\n",
    "$$tan\\left(\\frac{FOV}{2}\\right) = \\frac{w/2}{f} \\rightarrow FOV = arctan\\left(\\frac{w/2}{f}\\right)*2 = arctan\\left(\\frac{640/2}{738}\\right)*2 = 46.88\\degree$$\n",
    "\n",
    "Se obtiene un FOV horizontal de 46.88 grados.\n",
    "\n",
    "Ahora, se calculará el vertical (las fotografías tomadas por la cámara tienen un alto de 360 píxeles):\n",
    "\n",
    "$$tan\\left(\\frac{FOV}{2}\\right) = \\frac{w/2}{f} \\rightarrow FOV = arctan\\left(\\frac{w/2}{f}\\right)*2 = arctan\\left(\\frac{360/2}{738}\\right)*2 = 27.4\\degree$$\n",
    "\n",
    "Se obtiene un FOV vertical de 27.4 grados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77406102-7d6a-4764-a011-a137954b2da0",
   "metadata": {},
   "source": [
    "## Ángulo entre 2 puntos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e5fbb-7281-41db-81ac-d1b720ef4346",
   "metadata": {},
   "source": [
    "Para calcular el ángulo que forman los rayos ópticos correspondientes  a dos píxeles marcados con el ratón, se debe de tener en cuenta la imagen siguiente, perteneciente a los apuntes de la asignatura:\n",
    "\n",
    "<img src=\"MAPA/img/Imagen-angulo-puntos-material-asignatura.png\" style=\"width:40%\"/>\n",
    "\n",
    "Como se puede observar en la imagen, si se tiene la distancia focal, se puede obtener el vector que va hasta un punto de la imagen. Cogiendo el inicio del vector como (0,0,0), se tiene:\n",
    "\n",
    "- El desplazamiento en x: Es la coordenada x del punto en la imagen (en píxeles) menos la mitad del ancho de la imagen (en píxeles) ya que el inicio del vector se encuentra en el centro. \n",
    "- El desplazamiento en y: Se calcula de forma similar al anterior, con la coordenada en y del punto en la imagen menos la mitad del alto de la imagen (el inicio del vector se encuentra en el centro).\n",
    "- El desplazamiento en z: Es la distancia focal (en píxeles) de la imagen. \n",
    "\n",
    "Una vez obtenidos los dos vectores de dos puntos, se puede calcular el ángulo que estos forman con la fórmula siguiente:\n",
    "\n",
    "$$\\cos(\\alpha) = \\frac{\\vec{u} \\cdot \\vec{v}}{|\\vec{u}| |\\vec{v}|} \\rightarrow \\alpha = \\arccos{\\frac{\\vec{u} \\cdot \\vec{v}}{|\\vec{u}| |\\vec{v}|}}$$\n",
    "\n",
    "Y con todo esto, se obtiene el ángulo ($\\alpha$) entre los vectores (los rayos ópticos correspondientes a dos píxeles marcados con el ratón)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e683f-53f0-4e0e-9aa9-1e7bf3db8dfd",
   "metadata": {},
   "source": [
    "## Código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db695764-0b45-4be6-9196-1be9c5d61bab",
   "metadata": {},
   "source": [
    "Se ha modificado el código de `medidor.py` para añadir una función que calcula el ángulo entre los vectores dados sus puntos en la imagen. Se ha almacenado en variables la distancia focal, el alto y el ancho de la cámara (en píxeles) de forma que se pueda calcular el ángulo de la forma anteriormente explicada.\n",
    "\n",
    "Después, se ha añadido al texto mostrando la distancia entre los puntos, los ángulos que estos forman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffc11a-caa9-46c4-a97c-d69e7467dd10",
   "metadata": {},
   "source": [
    "## Cámara usada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4c336-e6c0-4fba-88b8-ed1e67f1ed64",
   "metadata": {},
   "source": [
    "Al utilizar la distancia focal y el FOV de la cámara para realizar el ejercicio, es importante indicar la cámara que se utiliza. Se ha cogido como referencia la cámara de un portátil Acer Aspire 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b890f28-eb90-4d94-a935-5a9ce8ecc423",
   "metadata": {},
   "source": [
    "## Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52555f-c12b-49f9-9719-7ff42acd3e62",
   "metadata": {},
   "source": [
    "*Utiliza el script para encontrar mediante una construcción geométrica la posición aproximada en un mapa desde la que se ha tomado una imagen en la que se ven algunos puntos característicos.*\n",
    "\n",
    "Se ha realizado un notebook en el que se puede observar el cálculo de la posición desde la que se ha tomado una fotografía dados dos puntos característicos. Este notebook se encuentra en *ejercicioMAPA.ipynb*.\n",
    "\n",
    "Las imágenes que se encuentran en la carpeta *img* son imágenes usadas en el notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404edcb-81d5-4a8e-928d-05cebb5593da",
   "metadata": {},
   "source": [
    "## Bibliografía usada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fc678-9423-425f-8ba0-1c9876ddd4f6",
   "metadata": {},
   "source": [
    "[Material de la asignatura](https://github.com/albertoruiz/umucv/blob/master/notebooks/imagen.ipynb)\n",
    "\n",
    "[Entender qué es la distancia focal](https://www.sony.es/electronics/support/articles/00267921)\n",
    "\n",
    "[Entender qué es un sensor](https://www.blogdelfotografo.com/tipos-caracteristicas-ventajas-sensores-camaras-fotos/)\n",
    "\n",
    "[Teorema de Thales](https://www.superprof.es/apuntes/escolar/matematicas/geometria/basica/triangulos-en-posicion-de-thales.html)\n",
    "\n",
    "[numpy dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)\n",
    "\n",
    "[numpy norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html#numpy-linalg-norm)\n",
    "\n",
    "[numpy arccos](https://numpy.org/doc/stable/reference/generated/numpy.arccos.html)\n",
    "\n",
    "[numpy degrees](https://numpy.org/doc/stable/reference/generated/numpy.degrees.html)\n",
    "\n",
    "[numpy array](https://numpy.org/doc/stable/reference/generated/numpy.array.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf547133-3858-4961-829a-693e74fa91fb",
   "metadata": {},
   "source": [
    "# FILTROS II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664eec2-ecf5-49d7-8754-3299c2674e97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e744d6-edaf-498c-880b-1301ef9d607f",
   "metadata": {
    "tags": []
   },
   "source": [
    "**a.** Comprueba la propiedad de \"cascading\" del filtro gaussiano.\n",
    "\n",
    "**b.** Comprueba la propiedad de \"separabilidad\" del filtro gaussiano. \n",
    "\n",
    "**c.** Implementa en Python dede cero (usando bucles) el algoritmo de convolución con una máscara general y compara su eficiencia con la versión de OpenCV. \n",
    "\n",
    "**c.** Impleméntalo en C y haz un \"wrapper\" para utilizarlo desde Python (consulta al profesor). \n",
    "\n",
    "**d)** Implementa el box filter con la imagen integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee389ae-f7d0-46ae-b8a8-98fc2132111e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Librerías usadas\n",
    "import numpy             as np\n",
    "import cv2               as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal      as signal\n",
    "import time\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aa438c-0749-446e-bde9-8943765b4839",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apartado a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22596a-a661-49f2-b91f-8f990ab49c1f",
   "metadata": {},
   "source": [
    "Para que se entienda de mejor forma qué es la propiedad cascading, se va a mostrar una imagen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369523b-5354-46e5-996b-90217e1e24cd",
   "metadata": {},
   "source": [
    "<img src=\"FILTROSII/img/gaussiano-desv-est.png\" style=\"width:50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60b9b7-adef-460f-a8ec-047b551972f6",
   "metadata": {},
   "source": [
    "La propiedad cascading indica que si se aplican dos gaussianas, con desviaciones estandar $\\sigma_1$ y $\\sigma_2$, a una imagen esta tiene el mismo resultado que aplicarle una gaussiana con desviación estándar $\\sqrt{\\sigma_1^2 + \\sigma_2^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238bcf73-5488-43d9-9ff2-a8efe4e734d2",
   "metadata": {},
   "source": [
    "Se procede a mostrar su funcionamiento mediante un código en Python, el el que $\\sigma_1 = 4$ y $\\sigma_2 = 3$, por lo que $\\sqrt{\\sigma_1^2 + \\sigma_2^2} = 5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb3522-1a3f-4a41-9ab8-4b66f5041e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Se lee la imagen y se pasa a escala de grises\n",
    "imagen = cv.imread('FILTROSII/img/foto.png')\n",
    "imagen = cv.cvtColor(imagen, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Se aplica a la imagen la gaussiana con sigma 4 y después con sigma 3\n",
    "imagen_2_gaussianas = cv.GaussianBlur(imagen, (0,0), 4) # 4\n",
    "imagen_2_gaussianas = cv.GaussianBlur(imagen_2_gaussianas, (0,0), 3) # 3\n",
    "\n",
    "# Se aplica a la imagen la gaussiana con sigma 5 (raiz cuadrada(4^2 + 3^2) = 5)\n",
    "imagen_1_gaussiana = cv.GaussianBlur(imagen, (0,0), 5)\n",
    "\n",
    "# Se muestran las imágenes y el mse\n",
    "print(\"mse: \", sklearn.metrics.mean_squared_error(imagen_1_gaussiana, imagen_2_gaussianas))\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1); plt.imshow(imagen_2_gaussianas, cmap='gray'); plt.axis(\"off\"); plt.title(\"2 gaussianas\");\n",
    "plt.subplot(1,2,2); plt.imshow(imagen_1_gaussiana, cmap='gray'); plt.axis(\"off\"); plt.title(\"1 gaussiana\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332f36a-79e1-4c2a-9a1b-091bf2ccfbab",
   "metadata": {},
   "source": [
    "Como se puede observar,  el error cuadrático medio obtenido es 0.13614145052272142 porque las imágenes son muy similares. Esto se corresponde con lo dicho con anterioridad, ya que las imágenes aplicando dos gaussianas o la correspondiente a estas dos es prácticamente la misma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ebc40-de83-41f5-b497-a771ec03348a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apartado b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d20ee7-26b7-4942-89ec-bcea87f4b64b",
   "metadata": {},
   "source": [
    "El filtro gaussiano sirve para suavizar una imagen. Este se puede aplicar mediante una máscara de 3x3, de forma que se suaviza tomando en cuenta los vecinos del píxel.\n",
    "\n",
    "Sin embargo, este también se puede aplicar mediante dos máscaras, una de 3x1 y otra de 1x3, de forma que en primer lugar los píxeles se mezclan con sus vecinos en vertical, y después su resultado mezcla sus vecinos en horizontal (o viceversa). Esto es la propiedad de separabilidad, las máscaras en 2 dimensiones aplicadas a una imagen también se pueden aplicar usando dos máscaras de 1 dimensión a la imagen de forma consecutiva. \n",
    "\n",
    "Se muestra de forma gráfica para que se entienda mejor. La siguiente máscara es una máscara 3x3 que aplica el filtro gaussiano (se multiplica por 1/16 porque la suma de los valores de la máscara es $1*4+2*4+4=16$):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b414df-625f-4707-95df-2905a2733916",
   "metadata": {
    "tags": []
   },
   "source": [
    "\\begin{equation}\n",
    "\\frac{1}{16}*\\begin{pmatrix}\n",
    "1 & 2 & 1\\\\\n",
    "2 & 4 & 2\\\\\n",
    "1 & 2 & 1\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\frac{1}{16} & \\frac{1}{8} & \\frac{1}{16}\\\\\n",
    "\\frac{1}{8} & \\frac{1}{4} & \\frac{1}{8}\\\\\n",
    "\\frac{1}{16} & \\frac{1}{8} & \\frac{1}{16}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a9cb6-b5ce-43f2-bfc0-790491853357",
   "metadata": {
    "tags": []
   },
   "source": [
    "Si se aplican las siguientes máscaras sobre la imagen, se genera el mismo resultado (se multiplica por $1/4$ por la misma razón que antes se multiplicaba por $1/16$, y es que los valores de las máscaras son $1*2+2=4$):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8dd86-f733-4395-b9b3-8428773d854d",
   "metadata": {
    "tags": []
   },
   "source": [
    "\\begin{equation}\n",
    "\\frac{1}{4}*\\begin{pmatrix}\n",
    "1\\\\\n",
    "2\\\\\n",
    "1\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\frac{1}{4}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{4}\n",
    "\\end{pmatrix}\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{4}*\\begin{pmatrix}\n",
    "1 & 2 & 1\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa918b76-ca07-427f-a98c-69c8482c7a25",
   "metadata": {
    "tags": []
   },
   "source": [
    "Esto ocurre porque \\begin{equation} \\begin{pmatrix}\n",
    "\\frac{1}{4}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{4}\n",
    "\\end{pmatrix} * \\begin{pmatrix}\n",
    "\\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\frac{1}{16} & \\frac{1}{8} & \\frac{1}{16}\\\\\n",
    "\\frac{1}{8} & \\frac{1}{4} & \\frac{1}{8}\\\\\n",
    "\\frac{1}{16} & \\frac{1}{8} & \\frac{1}{16}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9694f32-993e-46c2-8696-2e5073035dad",
   "metadata": {},
   "source": [
    "Se aplica el filtro con la máscara de 3x3 y con las otras máscaras a 2 imágenes, y se comprueba el mse obtenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f00b0-5525-4094-b31f-8163fcfc1bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se leen las imágenes y se pasan a escala de grises\n",
    "imagenMonte = cv.imread('FILTROSII/img/monte.jpg')\n",
    "imagenMonte = cv.cvtColor(imagenMonte, cv.COLOR_BGR2GRAY)\n",
    "imagenJuego = cv.cvtColor(cv.imread('FILTROSII/img/foto.png'), cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Se aplica la máscara 3x3\n",
    "mascara3x3 = np.array([[1,2,1],[2,4,2],[1,2,1]])\n",
    "mascara3x3 = mascara3x3/np.sum(mascara3x3)\n",
    "imagen_3x3_monte = cv.filter2D(imagenMonte,-1,mascara3x3)\n",
    "imagen_3x3_juego = cv.filter2D(imagenJuego,-1,mascara3x3)\n",
    "\n",
    "# Se aplican las máscaras 3x1 y 1x3\n",
    "mascara1 = np.array([[1],[2],[1]])\n",
    "mascara1 = mascara1/np.sum(mascara1)\n",
    "mascara2 = np.array([1,2,1])\n",
    "mascara2 = mascara2/np.sum(mascara2)\n",
    "imagen_2_mascaras_monte = cv.filter2D(cv.filter2D(imagenMonte, -1, mascara1),-1,mascara2)\n",
    "imagen_2_mascaras_juego = cv.filter2D(cv.filter2D(imagenJuego, -1, mascara1),-1,mascara2)\n",
    "\n",
    "# Se mira el mse\n",
    "print(\"mse imágenes juego: \", sklearn.metrics.mean_squared_error(imagen_3x3_juego, imagen_2_mascaras_juego))\n",
    "print(\"mse imágenes monte: \", sklearn.metrics.mean_squared_error(imagen_3x3_monte, imagen_2_mascaras_monte))\n",
    "\n",
    "# Se dibujan las imágenes obtenidas\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,4,1); plt.imshow(imagen_3x3_juego, cmap='gray'); plt.axis(\"off\"); plt.title(\"3x3 juego\");\n",
    "plt.subplot(1,4,2); plt.imshow(imagen_2_mascaras_juego, cmap='gray'); plt.axis(\"off\"); plt.title(\"2 máscaras juego\");\n",
    "plt.subplot(1,4,3); plt.imshow(imagen_3x3_monte, cmap='gray'); plt.axis(\"off\"); plt.title(\"3x3 monte\");\n",
    "plt.subplot(1,4,4); plt.imshow(imagen_2_mascaras_monte, cmap='gray'); plt.axis(\"off\"); plt.title(\"2 máscaras monte\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d213cc-3776-47de-8c46-3cca3fa0d4e6",
   "metadata": {},
   "source": [
    "Como se ha podido observar, el mse obtenido es de 16.98 en las imágenes del juego y de 3.67 en las imágenes del monte. Por lo tanto, aplicar un filtro gaussiano (3x3) o este separado en 2 filtros (1x3 y 3x1), dan imágenes muy similares. El error obtenido, como se ha podido observar depende de la imagen escogida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb192bc-ad87-42ed-88c1-f25b642aa72f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apartado c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b16a3b-84d2-4b98-9373-33ad44280d09",
   "metadata": {},
   "source": [
    "En primer lugar, se implementa el algoritmo de convolución con una máscara general (array de numpy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c20852-ac6e-4252-8ebc-32bcf6774263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv(img, array):\n",
    "    H,W = img.shape\n",
    "    resultado = img.copy()\n",
    "    array_h, array_w = array.shape\n",
    "    array_mitad_h = (array_h-1)//2\n",
    "    array_mitad_w = (array_w-1)//2\n",
    "    \n",
    "    # Recorre todos los píxeles de la imagen\n",
    "    for h in range(0,H-1):\n",
    "        for w in range(0,W-1):\n",
    "            # Por cada píxel\n",
    "            pixel_resultante = 0\n",
    "            # Recorre los vecinos del píxel (array)\n",
    "            for vecino_h in range(-array_mitad_h,array_mitad_h+1): \n",
    "                for vecino_w in range(-array_mitad_w,array_mitad_w+1):\n",
    "                    # Posición del vecino en la imagen: [i, j]\n",
    "                    i = vecino_h+h \n",
    "                    j = vecino_w+w\n",
    "                    # Si se ha salido de la imagen, se coge el píxel del borde de la imagen\n",
    "                    if i < 0:\n",
    "                        i = 0\n",
    "                    elif i >= H:\n",
    "                        i = H-1\n",
    "                    if j < 0:\n",
    "                        j = 0\n",
    "                    elif j >= W:\n",
    "                        j = W-1\n",
    "                    # pixel_resultante += El valor del píxel vecino en la imagen por el valor que tiene ese vecino en la máscara\n",
    "                    pixel_resultante +=  img[i,j]*array[vecino_h+array_mitad_h][vecino_w+array_mitad_w]\n",
    "            resultado[h,w] = pixel_resultante\n",
    "    return resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcca430-33be-4251-b689-09ebcf55604d",
   "metadata": {},
   "source": [
    "Se ha realizado el filtro dando por hecho que la imagen sólo tiene un valor por píxel (blanco y negro) para que la multiplicación de cada píxel por el valor que corresponda de la máscara se realice de forma sencilla. A su vez, también se ha dado por hecho que las filas y columnas del array de entrada (máscara) son impares.\n",
    "\n",
    "Se realiza un bucle por todos los píxeles de la imagen, y se le asigna a cada píxel la suma de los valores de los vecinos (y del píxel en sí) por el valor de la posición de cada uno en el array de la máscara. Si un vecino no existe (píxel en el borde de la imagen), se coje el píxel más cercano a este."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590e96d-dd87-4fb5-ad04-cc8dd8d0caac",
   "metadata": {},
   "source": [
    "Una vez implementado, se ejecuta con una imagen y una máscara. Se presenta la imagen original a la izquierda para que se vea el efecto del filtro.\n",
    "\n",
    "Se ha utilizado un filtro de los [apuntes de la asignatura](https://github.com/albertoruiz/umucv) ya que se ha considerado interesante el efecto de este, al verse de forma rápida si el filtro realiza lo que debería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442d1f2-5904-4871-9bcc-a9043244c0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se lee la imagen y se pasa a escala de grises\n",
    "imagen = cv.imread('FILTROSII/img/foto.png')\n",
    "imagen = cv.cvtColor(imagen, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Se crea la máscara (ker) sacada de los apuntes de la asignatura\n",
    "ker = np.zeros([11,11])\n",
    "ker[0,0] = 1\n",
    "ker[10,10] = 1\n",
    "ker = ker/np.sum(ker)\n",
    "\n",
    "# Se dibujan la imagen sin modificar en blanco y negro (izquierda), y la imagen pasada por el algoritmo de convolución (derecha)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1); plt.imshow(imagen, cmap='gray'); plt.axis(\"off\");\n",
    "plt.subplot(1,2,2); plt.imshow(conv(imagen, ker), cmap='gray'); plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafc095-b481-4d1d-bdb9-590efe7eb28a",
   "metadata": {},
   "source": [
    "Se calcula el tiempo que tarda en ejecutarse el algoritmo de convolución con la imagen y la máscara anteriores, tanto con el algoritmo creado (conv) como con el ofrecido por OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2638b-a2a4-40f1-a602-13ad9fc7d165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "conv(imagen, ker)\n",
    "fin = time.time()\n",
    "print(\"Tiempo del algoritmo implementado = \", fin-inicio)\n",
    "\n",
    "inicio = time.time()\n",
    "cv.filter2D(imagen,-1,ker)\n",
    "fin = time.time()\n",
    "print(\"Tiempo del algoritmo de OpenCV = \", fin-inicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ace61-a054-4d2d-9c67-8305a377aec6",
   "metadata": {},
   "source": [
    "Considerando que el tiempo de ejecución puede variar según múltiples factores como el hardware, indico los tiempos que me indicó al ejecutar la versión implementada por mí y la de OpenCV para poder comentarlo.\n",
    "\n",
    "Mi implementación tardó 50.3 segundos, en contraste de la versión de OpenCV que indicó 0.0.\n",
    "\n",
    "Esto proporciona una idea de lo importante que son las librerías de código en la programación, dado que estas se encuentran muy optimizadas (sobre todo las populares)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46c6dc-f2a4-437b-ae05-4fdf0a6bc857",
   "metadata": {},
   "source": [
    "# Apartado d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98fa93-844f-4245-af41-2fd9af7428a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Para este apartado, me he basado en la explicación del siguiente vídeo de youtube https://www.youtube.com/watch?v=5ceT8O3k6os, que explica qué son las imágenes integrales de forma muy clara y concisa (5 minutos, 28 segundos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1c0c8-3fa8-42d7-b9a3-d677faeff0b1",
   "metadata": {},
   "source": [
    "Las **imágenes integrales** se calculan a partir de las imágenes. Cada píxel de la imagen integral tiene como valor la suma de todos los valores de los píxeles superiores (y misma fila) a la derecha (y misma columna) de este píxel en la imagen original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7a7c6-c0a9-4289-b5e5-6fb6bc5763a8",
   "metadata": {},
   "source": [
    "De esta forma, se procede a hacer un método que calcule la imagen integral a partir de la imagen original (como en casos anteriores, se supone una imagen en escala de grises)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d7dc5-1d9e-4506-b228-64353d7bef5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def integral(imagen):\n",
    "    integral = np.zeros_like(imagen, np.int64)\n",
    "    H, W = imagen.shape\n",
    "    # h=0\n",
    "    integral[0,0] = imagen[0,0]\n",
    "    for w in range(1,W):\n",
    "        integral[0,w] = integral[0,w-1] + imagen[0,w]\n",
    "    # w=0\n",
    "    for h in range(1,H):\n",
    "        integral[h,0] = integral[h-1,0] + imagen[h,0]\n",
    "    # h!=0 w!=0\n",
    "    for h in range(1,H):\n",
    "        for w in range(1, W):\n",
    "            integral[h,w] = integral[h-1,w] + integral[h,w-1] - integral[h-1, w-1] + imagen[h,w]\n",
    "    return integral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22dcfe-8d03-45b6-9289-4527fd351052",
   "metadata": {},
   "source": [
    "Se ha seguido un método donde primero se calculan los píxeles con h=0 y con w=0, ya que estos son la suma de ese píxel en la imagen original más el anterior ya calculado en la fila/columna. Se enseña una imagen para mejor comprensión:\n",
    "\n",
    "<img src=\"FILTROSII/img/imagen-integral-1.png\" style=\"width:25%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3b299-f5af-4c2a-9fe1-1a76c796af1f",
   "metadata": {},
   "source": [
    "En la imagen integral, el valor del píxel 1 es el píxel 1 en la imagen original, el píxel 2 es la suma del 1 y 2 de la imagen original (píxel 2 en la original y 1 del integral), el píxel 3 es la suma del 1, 2 y 3 (es decir, la suma del píxel 2 en la integral y del 1 en la original), ...\n",
    "\n",
    "A su vez, el valor del píxel 8 en la integral es el valor del pixel 1 y el 8 en la original, el valor del píxel 9 en la integral es el valor del píxel 9 en la original y del 8 en la integral, ...\n",
    "\n",
    "Por esta razón se ha usado dos bucles para calcular estas primera fila/columna, sumando el píxel de la original y el anterior de la integral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f8bc0-d0d0-4afa-95bd-286854a298fe",
   "metadata": {},
   "source": [
    "Posteriormente, se ha pasado al resto de la imagen. Para calcular estos píxeles se usa el valor de los píxeles de el de arriba, el de la izquierda, y el de arriba a la izquierda. Como se hace después de calcular la primera fila y columna, estos píxeles siempre van a existir en la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afd97e-0a0c-44f6-8d97-81e1b0a89aee",
   "metadata": {
    "tags": []
   },
   "source": [
    "Se hace un dibujo para que se entienda mejor:\n",
    "\n",
    "<img src=\"FILTROSII/img/imagen-integral-2.png\" style=\"width:25%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec1c39-87f2-4725-9dde-d0e878fbe4ea",
   "metadata": {},
   "source": [
    "El valor del píxel amarillo en la imagen integral es la suma de todos los valores de los píxeles coloreados que aparecen, en la imagen original. \n",
    "\n",
    "Para hacer esto, en primer lugar se hace un bucle de arriba a abajo y de izquierda a derecha, de modo que se tengan los píxeles de arriba y de la izquierda ya calculados en la imagen integral para calcular el siguiente píxel de esta imagen integral.\n",
    "\n",
    "Dentro del bucle, para cada píxel (por ejemplo el que aparece en amarillo), se hace la suma del píxel de encima en la imagen integral (es la suma de los valores de los píxeles morados) y del píxel de la izquierda en la imagen integral (es la suma de los valores de los píxeles azules). Esto genera que los píxeles tanto morados como azules se hayan sumados dos veces, por lo que se resta el píxel de arriba a la izquierda en la imagen integral (es la suma de los píxeles en naranja, que como se ve también son los píxeles azules y morados que se han sumados dos veces)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10bea9c-98be-411c-992e-c15bc8f72429",
   "metadata": {},
   "source": [
    "Una vez obtenida la imagen integral, se debe de hacer el **filtro box** con la imagen integral. El filtro box puede tener un número variables de filas y columnas, por lo que se va a implementar de forma que se puedan indicar las filas y columnas de la imagen (deben ser ambas impares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0321b01-dd82-4900-a616-82b0b1f07f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se pasan como parámetro las filas, columnas, y la imagen integral\n",
    "def filtroBox(imagen, filas, columnas):\n",
    "    # Si el número de filas o columnas es par, devolver la imagen tal cual e indicar que el número de filas/columnas es par y esto no se puede\n",
    "    if filas%2 == 0 or columnas%2 == 0:\n",
    "        print(\"filas%2 == 0 or columnas%2 == 0\")\n",
    "        return imagen\n",
    "    imagenBox = np.zeros_like(imagen, np.uint8)\n",
    "    H, W = imagen.shape\n",
    "    for h in range(0, H):\n",
    "        for w in range(0,W):\n",
    "            fila_inicio = h+filas//2 \n",
    "            columna_inicio = w+columnas//2 \n",
    "            fila_fin = fila_inicio - filas \n",
    "            columna_fin = columna_inicio - columnas\n",
    "            # Si la fila o columna del inicio están fuera de la imagen\n",
    "            if fila_inicio >= H:\n",
    "                fila_inicio = H-1\n",
    "            if columna_inicio >= W:\n",
    "                columna_inicio = W-1\n",
    "            # Si la fila o columna del fin están fuera de la imagen\n",
    "            if fila_fin < 0:\n",
    "                if columna_fin < 0:\n",
    "                    imagenBox[h,w] = imagen[fila_inicio, columna_inicio]\n",
    "                    imagenBox[h,w] = imagenBox[h,w]/((fila_inicio+1)*(columna_inicio+1))\n",
    "                else:\n",
    "                    imagenBox[h,w] = imagen[fila_inicio, columna_inicio] - imagen[fila_inicio, columna_fin]\n",
    "                    imagenBox[h,w] = imagenBox[h,w]/((fila_inicio+1)*(columna_inicio-columna_fin))\n",
    "            elif columna_fin < 0:\n",
    "                imagenBox[h,w] = imagen[fila_inicio, columna_inicio] - imagen[fila_fin, columna_inicio]\n",
    "                imagenBox[h,w] = imagenBox[h,w]/((fila_inicio-fila_fin)*(columna_inicio+1))\n",
    "            else:\n",
    "                imagenBox[h,w] = ((imagen[fila_inicio, columna_inicio] - imagen[fila_fin, columna_inicio] - imagen[fila_inicio, columna_fin] + imagen[fila_fin, columna_fin])/((fila_inicio-fila_fin)*(columna_inicio-columna_fin)))\n",
    "    return imagenBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96dbc82-693f-452b-9e7c-cd4943dda222",
   "metadata": {},
   "source": [
    "Para mostrar lo que hace el código, se muestra una imagen para que se vea de forma más clara:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6eed9-859d-47f0-87bd-770b76a25bc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"FILTROSII/img/imagen-integral-3.png\" style=\"width:25%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa168c-a5f9-45f7-b6b0-2f20cd24b6ea",
   "metadata": {},
   "source": [
    "Para conseguir la suma de los valores alrededor del píxel amarillo (3 filas y 3 columnas), se debe obtener la suma de los valores que sólo tienen color azul (incluido el amarillo).\n",
    "\n",
    "Para ello, con la imagen integral, se accede al píxel número 1 (suma de píxeles azules), y a su valor se le resta el valor del píxel 2 (suma de píxeles rosas) y del píxel 3 (suma de píxeles morados). De esta forma, se tiene que se han restado dos veces los valores que envuelve tanto el píxel 1 (azul) como el 2 (morado), por lo que se vuelven a sumar estos valores sumándole el píxel 4 (suma de valores en rojo, conincide con lo píxeles rosas y morados a la vez).\n",
    "\n",
    "De esta forma, se han obtenido la suma de valores de los píxeles que sólo son azules. Esto es lo que se realiza en el código de *filtroBox*, y sus variables son:\n",
    "- *fila_inicio*: La fila de la casilla 1\n",
    "- *fila_fin*: La fila de la casilla 4\n",
    "- *columna_inicio*: Columna de la casilla 1\n",
    "- *columna_fin*: Columna de la casilla 4\n",
    "\n",
    "Una vez obtenidos estos valores, se calcula el valor de cada píxel mediante la suma/resta de 4 valores. Esto se puede extender a distintos valores de filas y columnas, obteniendo siempre el píxel 1 como el de la esquina derecha-abajo del retángulo requerido, el píxel 4 como la esquina arriba-izquierda de las afueras del rectángulo, y los píxeles 2 y 3 se obtienen sabiendo las filas y columnas de los píxeles 1 y 4.\n",
    "\n",
    "Sin embargo, se tienen las siguientes excepciones:\n",
    "- fila_inicio >= H: Si la fila de inicio está fuera de la imagen, se guarda como fila_inicio la última fila, de forma que se tiene un rectángulo más pequeño al estar pegado a los bordes\n",
    "- columna_inicio >= W: Si la columna de inicio está fuera de la imagen, se guarda como columna_inicio la última columna, y al igual que en el anterior caso, se tiene un rectángulo más pequeño al estar pegado a los bordes\n",
    "- fila_fin < 0: Si la fila de fin está fuera de la imagen, ya no se resta el píxel 2 (no hay valores que restar). Por esa misma razón, tampoco se suma el píxel 4.\n",
    "- columna_fin < 0: Si la columna de fin está fuera de la imagen, ya no se resta el píxel 3 (no hay valores que restar). Por esa misma razón, tampoco se suma el píxel 4.\n",
    "- fila_fin < 0 y columna_fin < 0: Simplemente se accede al valor del píxel 1\n",
    "\n",
    "Una vez sumado, se tiene que dividir por el número de píxeles. No es filas\\*columnas por haber píxeles cuyo rectángulo es más pequeño por estar en los bordes, por lo que es:\n",
    "- vertical del rectángulo: El número de píxeles en vertical del rectángulo dependen de la fila de fin:\n",
    "    - fila_fin < 0: Entonces, el tamaño (celdas) en vertical del rectángulo es el valor de la fila de inicio (+1 por empezar a contar en 0)\n",
    "    - fila_fin >= 0: Entonces, el tamaño es la fila de inicio menos la fila de fin\n",
    "- horizontal del rectángulo: El número de píxeles en horizontal del rectángulo dependen de la columna de fin:\n",
    "    - columna_fin < 0: Ocurre lo mismo que con las filas. El tamaño en horizontal del rectángulo de celdas es el valor de la columna de inicio +1 (tambien se empieza a contar en 0).\n",
    "    - columna_fin >= 0: Entonces, el tamaño es la columna de inicio menos la de fin\n",
    "    \n",
    "Es importante recordar que la fila y columna de inicio se modificó para que estuviera dentro de la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb8cb2-138b-4ec8-aa8d-b5e08ea0b3d4",
   "metadata": {},
   "source": [
    "Una vez implementado el filtro box, se **muestra la imagen original y la pasada por el filtro** box (se hace con filas=5 y columnas=5, pero se puede modificar cambiando la línea en la que se aplica el filtro *filtroBox(integralImagen, 5,5)* modificando los dos últimos valores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509df284-a71f-4f7b-b027-890e6c0cd788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se lee la imagen y se pasa a escala de grises\n",
    "imagen = cv.imread('FILTROSII/img/foto.png')\n",
    "imagen = cv.cvtColor(imagen, cv.COLOR_BGR2GRAY)\n",
    "# Se obtiene la imagen integral y, se usa para obtener la imagen con el filtro de Box\n",
    "integralImagen = integral(imagen)\n",
    "filteredImagen = filtroBox(integralImagen, 5,5)\n",
    "\n",
    "# Se dibujan la imagen sin modificar en blanco y negro (izquierda), y la imagen con el filtro box aplicado (derecha)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1); plt.imshow(imagen, cmap='gray'); plt.axis(\"off\");\n",
    "plt.subplot(1,2,2); plt.imshow(filteredImagen, cmap='gray'); plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed8591-c768-4b2c-b0b1-c2189c945117",
   "metadata": {},
   "source": [
    "Por último, para **comprobar** que el filtro box **funciona de forma correcta**, se comprueba que la imagen original y la pasada por el filtro box con tamaño de fila y de columna 1 son iguales.\n",
    "\n",
    "Deben ser iguales porque el filtro box aplicado a una imagen da como valor en cada píxel la media de él y sus vecinos de la imagen original, y si se escoje 1 sola fila y columna, sólo se tiene en cuenta el pixel. Por ello, los valores de cada píxel en la imagen original y en la imagen a la que se aplicó el filtro box deben ser iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1735d90-8515-4267-8ed8-5051f9ecd560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imagen = cv.imread('FILTROSII/img/foto.png')\n",
    "imagen = cv.cvtColor(imagen, cv.COLOR_BGR2GRAY)\n",
    "integralImagen = integral(imagen)\n",
    "filteredImagen = filtroBox(integralImagen, 1,1)\n",
    "np.equal(imagen,filteredImagen).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558740a1-8137-4db3-b4a3-1e918fccbd18",
   "metadata": {},
   "source": [
    "*np.equal* devuelve un array de las mismas proporciones de los arrays pasados como parámetro donde si los valores correspondientes eran iguales en ambos arrays, en el devuelto hay True en esa posición, y si no, False. all sólo devuelve True si todos los valores del array son True. Si hay alguno a False, devuelve False.\n",
    "\n",
    "Al haber devuelto True, queda demostrado que las imágenes original, y pasada por el filtro box con fila y columna tamaño 1, son iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e70c5b-2e58-458b-b478-a6f0419fb2e9",
   "metadata": {},
   "source": [
    "## Bibliografía"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b016af-d6cf-4bb9-bcf2-ecf297b67a22",
   "metadata": {},
   "source": [
    "[Apuntes de la asignatura](https://github.com/albertoruiz/umucv)\n",
    "\n",
    "[ChatGPT para saber usar math.ceil y plt.figure para hacer plots de un número de imágenes variables](https://chat.openai.com/)\n",
    "\n",
    "[Imágenes integrales](https://www.youtube.com/watch?v=5ceT8O3k6os)\n",
    "\n",
    "[Crear array con ceros](https://numpy.org/doc/stable/reference/generated/numpy.zeros_like.html)\n",
    "\n",
    "[np.equal](https://numpy.org/doc/stable/reference/generated/numpy.equal.html)\n",
    "\n",
    "[función all](https://www.geeksforgeeks.org/python-all-function/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e67a17-4666-4fd3-9d96-60477731e5be",
   "metadata": {},
   "source": [
    "# POLYGON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920581d-9ff2-4dcb-b147-59e7be5da397",
   "metadata": {},
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b0b32-5ba5-4d57-b263-ad5c98c96f7b",
   "metadata": {},
   "source": [
    "Implementa un procedimiento para mejorar la aproximación poligonal obtenida por el método `cv.approxPolyDP` permitiendo vértices fuera del contorno de entrada. Da una medida de la calidad de la aproximación. Pruébalo construyendo un cuadrilátero a partir de la silueta de un carnet o una tarjeta con las esquinas redondeadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e179934-4984-428b-90b4-22489829a328",
   "metadata": {},
   "source": [
    "## Explicación general del procedimiento implementado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86176b31-8593-45eb-90fc-db066a81d98e",
   "metadata": {},
   "source": [
    "Para mejorar la aproximación de un contorno, permitiendo vértices fuera del contorno de entrada, se han realizado los siguientes pasos:\n",
    "\n",
    "1. Se ha ejecutado **cv.approxPolyDP**, de forma que se obtiene un contorno más simplificado.\n",
    "2. Como este método no funciona para esquinas redondeadas, se ha incluido un *factor de calidad* con un valor perteneciente a \\[0,1), de forma que **se eliminan las rectas** del contorno cuya **longitud sea demasiado pequeña** comparada con el resto (factor$*$longitud<longitud de la mayor línea).\n",
    "3. Una vez eliminadas las líneas demasiado pequeñas, para obtener las esquinas redondeadas, se calcula la **intersección entre las rectas** que se encontraban **conectadas a las rectas pequeñas que se han eliminado**, y se añade el punto de intersección al contorno, de forma que se obtiene la esquina que estas realizan.\n",
    "4. Una vez obtenido todo esto, se vuelve a realizar **cv.approxPolyDP**, de forma que se termina de simplificar el contorno.\n",
    "\n",
    "De esta forma, las rectas con un tamaño menor a un tanto por ciento de la recta más larga (el tanto por ciento se especifica por el *factor de calidad*), se consideran pertenecientes al redondeo de alguna esquina, por lo que se eliminan y se calcula la esquina correspondiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7225563-5d8b-4a87-9793-e2274cf8ee6b",
   "metadata": {},
   "source": [
    "## Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3eb2e2-87a5-462c-bbcc-972f576919da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from umucv.stream   import autoStream\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from umucv.contours import extractContours\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d311980-888e-4ef9-8553-f05318d81cd9",
   "metadata": {},
   "source": [
    "En primer lugar, se realiza un método para mejorar la aproximación del contorno pasado como parámetro (los pasos 2 y 3 de la explicación general). En vez de explicarlo en un fragmento de código, se explica en comentarios del código porque se considera que así se entiende mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73e655-49ec-4c8e-b976-347ba700de62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mejora la aproximación del contorno pasado como parámetro\n",
    "def mejorar_aproximacion(contorno, factor_precision=0.25):\n",
    "    # copia de contorno\n",
    "    contorno_parametro = np.array(contorno)\n",
    "    # pone el contorno, en formato (n, 2) siendo n el número de puntos del contorno\n",
    "    contorno = contorno.reshape(contorno.shape[0],2)\n",
    "\n",
    "    # max_dist = Máxima distancia entre puntos de la silueta\n",
    "    max_dist = 0.0\n",
    "    for i in range(len(contorno)):\n",
    "        max_dist = max(max_dist, distancia(contorno[i], contorno[i-1]))\n",
    "          \n",
    "    # puntos = Contorno con los puntos que pertenecen a rectas largas, y None para indicar que se añada una línea como intersección de las dos líneas largas\n",
    "    # que se queden a los lados de None (para redondear las esquinas)\n",
    "    puntos = []\n",
    "    j = 0\n",
    "    for i in range(1, len(contorno)-1, 1):\n",
    "        # Si el punto no está en ninguna línea \"larga\"\n",
    "        if ((distancia(contorno[i], contorno[i-1]) < factor_precision*max_dist) and (distancia(contorno[i], contorno[i+1]) < factor_precision*max_dist)):\n",
    "            # Añadir None para alargar las rectas de los lados\n",
    "            if j == 0 or puntos[j-1] is not None:\n",
    "                puntos += [None]\n",
    "                j += 1\n",
    "        # Si el punto pertenece a una línea \"larga\" por la izquierda pero no por la derecha\n",
    "        elif (distancia(contorno[i], contorno[i+1]) < factor_precision*max_dist):\n",
    "            puntos += [contorno[i]]\n",
    "            puntos += [None]\n",
    "            j += 2\n",
    "        # Si el punto pertenece a una línea larga por ambos lados\n",
    "        else:\n",
    "            puntos += [contorno[i]]\n",
    "            j += 1\n",
    "    \n",
    "    # El mismo concepto pero con los puntos de las esquinas\n",
    "    if (len(contorno) >= 2):\n",
    "        # Punto [0]: No pertenece a ninguna recta \"larga\"\n",
    "        if (distancia(contorno[0], contorno[1]) < factor_precision*max_dist) and (distancia(contorno[0], contorno[len(contorno)-1]) < factor_precision*max_dist):\n",
    "            if j == 0 or puntos[0] is not None:\n",
    "                puntos = [None] + puntos\n",
    "                j += 1\n",
    "        # Punto[0]: Pertenece a una recta \"larga\" por la izquierda pero no por la derecha\n",
    "        elif (distancia(contorno[0], contorno[1]) < factor_precision*max_dist):\n",
    "            if j == 0 or puntos[0] is not None:\n",
    "                puntos = [contorno[0]] + [None] + puntos\n",
    "                j += 2\n",
    "            else:\n",
    "                puntos = [contorno[0]] + puntos\n",
    "                j += 1\n",
    "        # Punto[0]: Pertenece a una línea larga por ambos lados\n",
    "        else:\n",
    "            puntos = [contorno[0]] + puntos\n",
    "            j += 1\n",
    "        # Punto[ultimo]: No pertenece a ninguna recta \"larga\"\n",
    "        if (distancia(contorno[len(contorno)-2], contorno[len(contorno)-1]) < factor_precision*max_dist) and (distancia(contorno[0], contorno[len(contorno)-1]) < factor_precision*max_dist):\n",
    "            if j == 0 or puntos[j-1] is not None:\n",
    "                puntos += [None]\n",
    "                j += 1\n",
    "        # Punto[ultimo]: Pertenece a una recta \"larga\" por la izquierda pero no por la derecha\n",
    "        elif (distancia(contorno[len(contorno)-1], contorno[0]) < factor_precision*max_dist):\n",
    "            if j == 0 or puntos[0] is not None:\n",
    "                puntos = puntos + [contorno[len(contorno)-1]] + [None]\n",
    "                j += 2\n",
    "            else:\n",
    "                puntos = puntos + [contorno[len(contorno)-1]]\n",
    "                j += 1\n",
    "        # Punto[ultimo]: Pertenece a una línea larga por ambos lados\n",
    "        else:\n",
    "            puntos = puntos + [contorno[len(contorno)-1]]\n",
    "            j += 1\n",
    "        # Si se ha quedado con None al principio y al final, se elimina uno de ellos para evitar tener dos None seguidos\n",
    "        if (puntos[0] is None and puntos[len(puntos)-1] is None):\n",
    "            puntos = puntos[:-1]\n",
    "\n",
    "    # Si la silueta tiene menos de 5 puntos, no hay esquinas a las que quitar el redondeo\n",
    "    if len(puntos) < 5:\n",
    "        return contorno_parametro\n",
    "    \n",
    "    # puntosFinal = Silueta con los puntos de \"puntos\" y en vez de None un punto que forma la intersección de las dos rectas que se quedan a los lados de None\n",
    "    # Si no hay intersección entre las rectas (son paralelas), se pone el punto medio entre los puntos de los extremos del punto a añadir\n",
    "    puntosFinal = np.array([])\n",
    "    for i in range(len(puntos)):\n",
    "        if puntos[i] is None:\n",
    "            # Puntos que forman las rectas anterior y posterior\n",
    "            r1_p1 = (i-1) % len(puntos)\n",
    "            r1_p1 = puntos[r1_p1]\n",
    "            r1_p2 = (i-2) % len(puntos)\n",
    "            r1_p2 = puntos[r1_p2]\n",
    "            r2_p1 = (i+1) % len(puntos)\n",
    "            r2_p1 = puntos[r2_p1]\n",
    "            r2_p2 = (i+2) % len(puntos)\n",
    "            r2_p2 = puntos[r2_p2]\n",
    "            # Rectas anterior y posterior\n",
    "            recta1 = np.cross(np.array([r1_p1[0], r1_p1[1], 1]),np.array([r1_p2[0], r1_p2[1], 1]))\n",
    "            recta2 = np.cross(np.array([r2_p1[0], r2_p1[1], 1]),np.array([r2_p2[0], r2_p2[1], 1]))\n",
    "            # Intersección de rectas\n",
    "            recta_h_interseccion = np.cross(recta1, recta2)\n",
    "            if recta_h_interseccion[2] == 0: # Si las rectas son paralelas, el punto medio\n",
    "                if len(puntosFinal) == 0: # Si es el primer punto\n",
    "                    puntosFinal = np.array([[int((r1_p2[0]+r2_p1[0])/2),int((r1_p2[1]+r2_p1[1])/2)]])\n",
    "                else:\n",
    "                    puntosFinal = np.append(puntosFinal, [[int((r1_p2[0]+r2_p1[0])/2),int((r1_p2[1]+r2_p1[1])/2)]], axis=0)\n",
    "            else: # Si las rectas no son paralelas, se añade el punto de intersección entre las rectas\n",
    "                if len(puntosFinal) == 0: # Si es el primer punto\n",
    "                    puntosFinal = np.array([[int(abs(recta_h_interseccion[0]/recta_h_interseccion[2])), int(abs(recta_h_interseccion[1]/recta_h_interseccion[2]))]])\n",
    "                else:\n",
    "                    puntosFinal = np.append(puntosFinal, [[int(abs(recta_h_interseccion[0]/recta_h_interseccion[2])), int(abs(recta_h_interseccion[1]/recta_h_interseccion[2]))]], axis=0)\n",
    "        # Si el punto no es None, se añade a la silueta\n",
    "        else:\n",
    "            if len(puntosFinal) == 0: # Si es el primer punto\n",
    "                puntosFinal = np.array([[puntos[i][0], puntos[i][1]]])\n",
    "            else:\n",
    "                puntosFinal = np.append(puntosFinal, [[puntos[i][0], puntos[i][1]]], axis=0)\n",
    "    return puntosFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0a571-6cbd-4d4a-8a1e-65a7d105a8c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Posteriormente, se guarda la distancia entre 2 puntos en un método (*distancia*), ya que esto se necesita para la función anterior (*mejorar_aproximacion*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19630f62-c20b-4721-9233-21fa0ec92a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calcular distancia entre 2 puntos\n",
    "def distancia(p1, p2):\n",
    "    return np.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972ef4e-15f0-44d2-bac9-9d25b34f6622",
   "metadata": {},
   "source": [
    "Se utiliza el método dado por el profesor en los [apuntes de la asignatura](https://github.com/albertoruiz/umucv/tree/master) para reducir los puntos de un contorno con la llamada a una función de OpenCV que lo realiza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511d299-ad7c-42c4-9342-64cbf6485eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reducimos una polilínea con una tolerancia dada\n",
    "def redu(c, eps=0.5):\n",
    "    red = cv.approxPolyDP(c,eps,True)\n",
    "    return red.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbd639-518d-47d2-8e0c-7540cf0082e8",
   "metadata": {},
   "source": [
    "A este método se pasa el contorno con los puntos, y como se dijo anteriormente, se reducen los puntos con la función de OpenCV, se mejora la aproximación de las esquinas permitiendo puntos fuera del contorno, y se vuelve a usar la función de OpenCV para volver a reducir los puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd811ab2-0710-4a2b-be43-c8fa8b0edd86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtramos una lista de contornos con aquellos\n",
    "# que quedan reducidos a n vértices con la precisión deseada.\n",
    "def polygons(cs,n,prec=1):\n",
    "    # Quedarse con la silueta reducida\n",
    "    rs = [ redu(c,prec) for c in cs ]\n",
    "    # Mejora la aproximación de la silueta\n",
    "    rs_mejorado = [mejorar_aproximacion(r, 0.25) for r in rs]\n",
    "    # Vuelve a reducir la silueta\n",
    "    rs_mejorado_reducido = [redu(r, prec) for r in rs_mejorado]\n",
    "    # Quedarse con las siluetas que tengan n vértices\n",
    "    resultado = [r for r in rs_mejorado_reducido if len(r) == n]\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f8384-8c5d-4ac7-a36a-db68662ec422",
   "metadata": {},
   "source": [
    "## Prueba con búsqueda de cuadriláteros en la imagen de un carnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1fbdd-b506-4853-ad2e-35a9c672b7ee",
   "metadata": {},
   "source": [
    "Se prueba la aproximación con la imagen de un carnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede0971-61bb-4ce3-ab15-a64fd3d81890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se guarda la imagen en escala de grises para que tenga 1 solo valor y no 3 por píxel (como es el caso de BGR).\n",
    "carnet = cv.imread('POLYGON/carnet-falso.jpg')\n",
    "# Se invierten los colores porque extractContours sólo extrae contornos de figuras oscuras sobre fondos claros\n",
    "carnet = cv.bitwise_not(carnet)\n",
    "carnet = cv.cvtColor(carnet,cv.COLOR_BGR2GRAY)\n",
    "# Se extraen los contornos\n",
    "cs = extractContours(carnet, minarea=10)\n",
    "# Se llama al método que mejora la aproximación de los contornos (reduce los puntos) y se queda solamente con aquellos que tras la reducción tienen 4 vértices\n",
    "good = polygons(cs, n=4, prec=3)\n",
    "# Se vuelve a leer la imagen para que se vean los contornos sobre la imagen original\n",
    "carnet = cv.imread('POLYGON/carnet-falso.jpg')\n",
    "# dibujamos en cyan todos los contornos interesantes\n",
    "cv.drawContours(carnet,[c.astype(int) for c in cs], -1, (0,255,255), 2, cv.LINE_AA)\n",
    "# dibujamos en rojo más grueso los cuadrados encontrados\n",
    "cv.drawContours(carnet,[c.astype(int) for c in good], -1, (255,0,0), 3, cv.LINE_AA)\n",
    "# Mostrar imagen con las líneas dibujadas\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(carnet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989e7d0-f5f7-41f9-b4fd-bda3e70fedad",
   "metadata": {},
   "source": [
    "Como se puede observar, se ha eliminado el redondeo de las esquinas, con vértices para estas fuera del contorno de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26dc58-157d-4e21-ae78-d8429f774f56",
   "metadata": {},
   "source": [
    "## Teoría utilizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec04e3-6059-4de5-83ea-a2fba2cbfba3",
   "metadata": {},
   "source": [
    "Para obtener las intersecciones de rectas, se han utilizado las coordenadas homogéneas, cuya explicación se encuentra en [los apuntes de la asignatura](https://github.com/albertoruiz/umucv/blob/96a0e8bcc9e95151c309ac48f743e4fd8a90a77c/notebooks/coordhomog.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4882f1d-fb76-4283-acd3-7f8449413fdf",
   "metadata": {},
   "source": [
    "A su vez, se ha realizado el código partiendo del programa [polygon0.py](https://github.com/albertoruiz/umucv/blob/96a0e8bcc9e95151c309ac48f743e4fd8a90a77c/code/polygon/polygon0.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878b86e-9ffc-4be9-bea0-49fdfd4d73a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MODEL3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fface-68d6-43f6-bab2-ef26e4856f0f",
   "metadata": {},
   "source": [
    "## Enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35768c76-31e6-4472-b899-3795cd8bea13",
   "metadata": {},
   "source": [
    "Utiliza alguna de las herramientas [colmap](https://colmap.github.io/) o [meshroom](https://meshroom-manual.readthedocs.io/en/latest/) para construir un modelo 3D de un objeto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e82423-8844-416f-a1fe-50a6defb0155",
   "metadata": {},
   "source": [
    "## Elección del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6aa826-cf56-4182-b0f3-281e0021802c",
   "metadata": {},
   "source": [
    "Se ha elegido la herramienta COLMAP para realizar el modelo, y meshLab para visualizarlo.\n",
    "\n",
    "Se han realizado fotografías a un objeto desde diferentes ángulos, de forma que se pueda reconstruir el objeto en 3D. A continuación, se ha creado el modelo 'sparse model' con COLMAP, utilizando las imágenes tomadas anteriormente. Esto ha generado un modelo.\n",
    "\n",
    "Después, se ha intentado realizar el modelo 'dense model' con COLMAP, pero esto no se ha logrado ya que la aplicación no ha devuelto ningún resultado (se ha realizado múltiples veces, se ha probado con otro objeto, y con el database que se ofrece en la página de COLMAP, pero sigue sin dar resultado). \n",
    "\n",
    "Por esta razón, se cree que por motivos de la capacidad del ordenador, no se puede crear un modelo 'dense model'. Para poder mostrar un modelo, se ha utilizado el modelo 'sparse model', aunque este no es tan completo al tratarse de puntos en lugar de una figura completa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ed70f-ceaf-49c6-9e56-0d7705b33fc9",
   "metadata": {},
   "source": [
    "## Creación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d2eea-b2ce-4d6b-85dd-f18f948fa99e",
   "metadata": {},
   "source": [
    "Se ha elegido una araña porque es un objeto sencillo con formas definidas, de forma que el programa pueda identificar bien los puntos en común entre las imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69555c4-1e51-4d33-a940-cb12d758d58f",
   "metadata": {},
   "source": [
    "Con las fotografías tomadas, se ha creado el siguiente modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc47ddf-97f4-4a08-88a4-ef32119eb8d1",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;\">\n",
    "    <img src=\"MODEL3D/img/modelo.png\" style=\"width:20%;\">\n",
    "    <img src=\"MODEL3D/img/modelo-arriba.png\" style=\"width:20%;\">\n",
    "    <img src=\"MODEL3D/img/lado-modelo.png\" style=\"width:20%;\">\n",
    "    <img src=\"MODEL3D/img/cara-modelo.png\" style=\"width:20%\">\n",
    "    <img src=\"MODEL3D/img/atras-modelo.png\" style=\"width:20%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03a9ae-3a5a-4f83-8c68-a2b949e7a808",
   "metadata": {},
   "source": [
    "El modelo se ve bien desde arriba. Sin embargo, si se ve desde el lado, no se aprecia bien la forma de la araña (en especial las patas de la izquierda y la cara). Desde atrás se observa bien la forma del cuerpo de la araña."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017fadf-676a-4145-a153-73ebd13e5be0",
   "metadata": {},
   "source": [
    "Si se quiere observar el modelo, este se encuentra con el nombre *modelo.ply*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa5d0d-9f7e-4ac6-b050-8c639db88d57",
   "metadata": {},
   "source": [
    "## Bibliografía"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbcd75f-34df-4c2f-901b-852447e6803b",
   "metadata": {},
   "source": [
    "[COLMAP](https://colmap.github.io/)\n",
    "\n",
    "[instalación COLMAP](https://github.com/colmap/colmap/releases)\n",
    "\n",
    "[Video sobre uso de COLMAP](https://www.youtube.com/watch?v=mUDzWCuopBo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
