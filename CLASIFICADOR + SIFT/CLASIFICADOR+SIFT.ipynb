{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa5c5f6",
   "metadata": {},
   "source": [
    "# CLASIFICADOR y SIFT\n",
    "\n",
    "Como SIFT se trata de una ampliación de CLASIFICADOR, se ha decidido incluir ambos en un mismo notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50fe399",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c56344",
   "metadata": {},
   "source": [
    "[Enunciados](#Enunciados)\n",
    "\n",
    "[Ficheros](#Ficheros)\n",
    "\n",
    "[Código](#Código)\n",
    "\n",
    "- [clasificadorConstructor.py](#clasificadorConstructor.py)\n",
    "- [clasificadorColeccion.py](#clasificadorColeccion.py)\n",
    "- [clasificador.py](#clasificador.py)\n",
    "\n",
    "[Conceptos teóricos](#Conceptos-teóricos)\n",
    "\n",
    "[Ejecución del código: Ejemplo de funcionamiento](#Ejecución-del-código:-Ejemplo-de-funcionamiento)\n",
    "\n",
    "[Tardanza del código](#Tardanza-del-código)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94b067",
   "metadata": {},
   "source": [
    "## Enunciados\n",
    "\n",
    "**Enunciado de CLASIFICADOR:**\n",
    "\n",
    "Prepara una aplicación sencilla de reconocimiento de imágenes. Debe admitir (al menos) dos argumentos:\n",
    "\n",
    "- `--models=<directorio>`, la carpeta donde hemos guardado un conjunto de imágenes de objetos o escenas que queremos reconocer.\n",
    "\n",
    "- `--method=<nombre>`, el nombre de un método de comparación.\n",
    "\n",
    "Cada fotograma de entrada se compara con los modelos utilizando el método seleccionado y se muestra información sobre el resultado (el modelo más parecido o probable, las distancias a los diferentes modelos, alguna medida de confianza, etc.). Implementa inicialmente un método basado en el \"embedding\" obtenido por [mediapipe](https://developers.google.com/mediapipe/solutions/vision/image_embedder) (`code/DL/embbeder`).\n",
    "\n",
    "**Enunciado de SIFT:**\n",
    "\n",
    "Añade al ejercicio CLASIFICADOR un método basado en el número de coincidencias de `keypoints` SIFT. Utilízalo para reconocer objetos con bastante textura (p. ej. carátulas de CD, portadas de libros, cuadros de pintores, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debab8a0",
   "metadata": {},
   "source": [
    "## Ficheros\n",
    "\n",
    "- Carpeta `code`: Aquí se encuentran los ficheros de código\n",
    "- Carpeta `images`: Aquí se encuentran imágenes de prueba para el clasificador.\n",
    "- Carpeta `imports`: Aquí se encuentran los ficheros importantes para el funcionamiento del programa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadb26a",
   "metadata": {},
   "source": [
    "## Código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d005cb",
   "metadata": {},
   "source": [
    "### clasificadorConstructor.py:\n",
    "\n",
    "En este fichero se encuentra la clase `Clasificador` cuyas subclases son los distintos métodos de clasificación que se pueden aplicar. \n",
    "\n",
    "Se crea un clasificador por cada imagen de ``models``, debido a que esto permite que cada instancia de subclase guarde transformaciones a la imagen en la inicialización de la instancia, y se haga una sola vez, en lugar de una vez por cada frame.\n",
    "\n",
    "De esta forma, en el método de inicialización (`\\_\\_init\\_\\_`), se pasa como parámetro el path a la imagen (\"imgPath\") y  el nombre de la imagen (\"nombreImg\"). Se guarda el nombre de la imagen, de forma que se pueda llamar al método `getNombreImg()` cada vez que este se necesite. \n",
    "\n",
    "Cada subclase tiene un método estático llamado \\textit{getMethod()} que devuelve el nombre del clasificador. Este es el nombre con el que se llama a ese clasificador concreto (`--method`) en los parámetros de entrada del programa. \n",
    "\n",
    "El método de clase `changeFrame` recibe como parámetro el frame actual. De esta forma, este método permite realizar las transformaciones necesarias al frame y guardarlas en variables de clase.\n",
    "\n",
    "Así, en lugar de hacer las transformaciones una vez por cada instancia, se hacen las transformaciones una vez por frame.\n",
    "\n",
    "El método llamado `similarity` devuelve la similaridad del frame actual (el que se guardó la última vez que se llamó a `changeFrame()`), con la imagen de la instancia. También devuelve el frame modificado según se quiera. Por ejemplo, en el método `skimageHog` se dibuja un rectángulo en el frame en la posición con mayor similaridad.\n",
    "\n",
    "### clasificadorColeccion.py:\n",
    "\n",
    "Contiene las subclases de la clase Clasificador. Cada subclase implementa \\textit{similarity}, \\textit{changeFrame}, y el constructor de la subclase. A su vez, cada una implementa el método `getMethod()` en el que devuelven el nombre del método.\n",
    "\n",
    "- **embedder**\n",
    "\n",
    "  Se inicializa la clase con el método `\\_\\_init\\_\\_`. Este método llama a un método de clase (`inicializoClase`) que crea un objeto `ImageEmbedder` con el modelo almacenado en `./embedder.tflite`. También se utiliza la imagen del clasificador para guardar su embedding (calculado con el modelo).\n",
    "\n",
    "  Es importante indicar que sólo se llamará a `inicializoClase` una vez (y no en todas las instancias), de forma que se comparte el modelo entre todas las instancias de esta subclase.\n",
    "\n",
    "  El método `changeFrame` guarda el frame pasado como parámetro y su embedding (calculado con el modelo).\n",
    "\n",
    "  En el método `similarity`, se utiliza un método de mediapipe que calcular la similaridad entre los embeddings del frame y de la imagen.\n",
    "\n",
    "  El frame se modifica mostrando la imagen y la similaridad obtenida, además del frame.\n",
    "\n",
    "- **skimageHog**\n",
    "\n",
    "  Se inicializa la clase con el método `\\_\\_init\\_\\_`, donde se guarda el nombre de la imagen de la clase, el histograma de orientaciones del gradiente (HOG) de esta imagen, la altura y anchura del HOG y el HOG aplanado (con el uso del método `flatten()`). \n",
    "\n",
    "  Dado que estas operaciones son muy lentas, guardar el HOG, y este aplanado, ayuda a no tener que calcularlo por cada frame, y por lo tanto que la aplicación resulte más rápida.\n",
    "\n",
    "  En el método `changeFrame` se guarda el HOG del frame, su altura y su anchura. No se guarda este aplanado porque se debe calcular en similarity, cómo se verá a continuación.\n",
    "\n",
    "  En el método `similarity` se calcula la distancia entre los HOGs de la imagen de la instancia y el frame almacenado.\n",
    "\n",
    "  La distancia entre HOGs de dos imágenes se calcula entre HOGs de igual tamaño, por lo que se calcula esta distancia entre el HOG de la imagen y cada una de las posibles partes del HOG del frame que tengan el mismo tamaño que el de la imagen. \n",
    "\n",
    "  Esto se hace mediante dos bucles que van desde el principio del HOG del frame (0,0) hasta la diferencia de altura o anchura de los HOGs (si se pasa de la diferencia, no se puede comparar con la imagen porque faltarían valores a la derecha/debajo del HOG del frame).\n",
    "\n",
    "  Esta distancia se calcula entre HOGs aplanados, de forma que resulte más cómoda su comparación. Una vez aplanado, bastaría con ir por cada bloque y sumar la diferencia de cada par de bloques del HOG, entre el número de bloques totales. Esta diferencia se calcula mirando la diferencia de la magnitud de los vectores de cada orientación.\n",
    "\n",
    "  De estas distancias, se busca la más pequeña, y se devuelve 1 - la distancia, de forma que cuanto más grande sea el valor, más parecidas son las imágenes.\n",
    "\n",
    "  Por esto el frame no se guarda aplanado, ya que se debe aplanar cada parte del HOG del frame para compararla con la imagen. Por ello, no sirve el HOG entero aplanado y se debe aplanar cada vez.\n",
    "\n",
    "  El frame en `similarity` se modifica, dibujando un rectángulo en la posición de la imagen con mayor similaridad. Se devuelve el frame modificado.\n",
    "\n",
    "  A su vez, se dibuja la distancia entre el frame y la imagen, y la imagen de la instancia en la esquina superior izquierda del frame.\n",
    "\n",
    "  Se devuelve este frame modificado, y 1 menos la distancia menor, de forma que cuanto más grande sea, más similares sean la imagen y la parte del frame seleccionada.\n",
    "\n",
    "- **sift**\n",
    "\n",
    "  Se inicializa la clase con el método `\\_\\_init\\_\\_`, que guarda el atributo \"bf\" que sirve para comparar los keypoints de dos imágenes y dar las dos mejores coincidencias para cada punto, y el atributo \"sift\" que sirve para detectar los keypoints de una imagen. Estos atributos se guardan en atributos de clase, para que todas las distintas instancias de la subclase las compartan.\n",
    "\n",
    "  También se guardan los keypoints y los descriptores de estos de la imagen cuyo nombre se pasa como parámetro, de forma que no se tiene que calcular por cada frame.\n",
    "\n",
    "  En el método `changeFrame` se utiliza \"sift\" para obtener los keypoints del frame pasado como parámetro, y sus descriptores.\n",
    "\n",
    "  En el método `similarity`, se obtienen los dos mejores matches de cada keypoint con \"bf\". Estos matches se calculan con los keypoints (sus descriptores) de la imagen de la instancia con el frame almacenado.\n",
    "\n",
    "  Posteriormente, se hace el test de ratio para quedarse solo con los mejores matches.\n",
    "\n",
    "  El test de ratio se basa en quedarse con la mejor coincidencia de keypoints solo si hay mucha diferencia entre el parecido de la mejor con el keypoint, y el parecido de la segunda mejor con el keypoint. De esta forma, se evita escoger coincidencias erróneas.\n",
    "\n",
    "  Después, se dibujan los matches en el frame y se escribe el número de keypoints que coincide. Se devuelve el frame modificado y la similaridad. \n",
    "\n",
    "  Para calcular la similaridad no se pueden devolver los keypoints que coinciden a secas, debido a que, si una imagen tuviese significativamente más keypoints que otra (por ejemplo 100 vs 10), es probable que termine teniendo más matches con el frame aunque el frame (o un trozo de este) se parezca más a la imagen con pocos keypoints. Para evitar esto, se divide el número de keypoints que coinciden entre el número de keypoints que tiene la imagen; de forma que se mira el porcentaje de keypoints de la imagen que se han encontrado en el frame.\n",
    "\n",
    "### clasificador.py:\n",
    "Es la clase principal del programa, es la que se ejecuta para que funcione el clasificador.\n",
    "\n",
    "En primer lugar, se recogen los parámetros de entrada del programa (directorio de modelos y método de comparación). Se comprueba que se han pasado los dos parámetros necesarios, que no se han escrito parámetros no conocidos, y que el directorio de modelos existe.\n",
    "\n",
    "Se obtiene el clasificador (de la clase Clasificador). Para ello, se recorren las subclases y se queda con la subclase cuyo atributo método coincide con el método pasado como parámetro. Si este no existe, se muestra un mensaje indicándolo y se cierra el programa.\n",
    "\n",
    "Cuando se tiene la subclase, se crea una instancia de esta por cada imagen del directorio de modelos.\n",
    "\n",
    "Se empieza a capturar el vídeo. Por cada frame que se captura, se llama al método `changeFrame` de la subclase escogida. Se llama al método `similarity` con todas las instancias de la subclase y se almacena el frame (modificado) devuelto por la instancia que devuelva la mayor similaridad. Por último, se muestra por pantalla el frame modificado.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ed682",
   "metadata": {},
   "source": [
    "## Conceptos teóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009bc69",
   "metadata": {},
   "source": [
    "**Gradiente**:\n",
    "\n",
    "El gradiente es un vector que indica hacia donde aumenta la luz. El gradiente de una imagen está compuesto por vectores que indican donde aumenta la luz por toda la imagen.\n",
    "\n",
    "Cuando se trabaja con una imagen en blanco y negro, los cambios entre zonas blancas y zonas negras suelen ser zonas de bordes. Por ello, los cambios de gradiente suelen indicar la presencia de un borde. Al tratar con objetos, el fijarse en sus bordes realmente es fijarse en la estructura general de este.\n",
    "\n",
    "Para utilizar el gradiente para clasificar objetos, en primer lugar se debe de suavizar la imagen para eliminar el ruido de la imagen que se toma del objeto y fijarse en el nivel justo de detalle, de forma que no se fije en los detalles pequeños sino en la estructura general de este.\n",
    "\n",
    "Tal y como se ha dicho, el gradiente ayuda a reconocer objetos. Esto ocurre si el objeto es rígido, o tiene deformaciones pequeñas, de forma que una vez obtenido su gradiente este no cambia (o no lo hace de forma significativa). Para comparar dos fotografías y conocer si son el mismo objeto, en lugar comparar los gradientes, se utilizan los histogramas de orientaciones del gradiente.\n",
    "\n",
    "**HOG**:\n",
    "\n",
    "El HOG, también llamado el histograma de orientaciones del gradiente, es un conjunto de histogramas locales sobre las orientaciones discretizadas del gradiente.\n",
    "\n",
    "Los vectores del gradiente, en lugar de representarlos con coordenadas x e y, se pueden representar de una forma polar con la magnitud del vector y su orientación. Esta forma de representarlos es muy útil, permitiendo la discretización de las orientaciones, dividiendolas en un cierto número (por ejemplo 16). En el caso de skimageHog, hay 8 orientaciones distintas.\n",
    "\n",
    "Esta discretización de las orientaciones permite identificar objetos aunque hayan pequeños cambios.\n",
    "\n",
    "Además, se crean histogramas locales de los vectores, de forma que se agrupan los vectores de píxeles cercanos y por cada agrupación se guardan las magnitudes totales de los vectores hacia cada orientación posible. Esta magnitud total en cada orientación se puede calcular de distintas formas. \n",
    "\n",
    "Una forma es sumar el número de vectores con esa orientación, pero esta forma no tiene en cuenta las diferencias de vectores, ya que no es lo mismo un vector que va de blanco a negro que un vector que va de un gris más claro a un gris más oscuro.\n",
    "\n",
    "Una buena forma de calcular la magnitud total en cada orientación es sumando la magnitud de los vectores (locales) que están en esa orientación.\n",
    "\n",
    "Una vez se tienen estos histogramas locales, se suelen normalizar. Esta normalización puede ser con sólo el histograma local. Sin embargo, es mejor normalizar cada histograma local teniendo en cuenta los histogramas locales cercanos a este.\n",
    "\n",
    "De esta forma, se termina consiguiendo un HOG (conjunto de histogramas locales) que permite identificar objetos aunque hayan pequeños cambios en estos. Es importante recordar que esto se debe realizar sobre las fotografías en blanco y negro.\n",
    "\n",
    "**SIFT**:\n",
    "\n",
    "Este método calcula los ``keypoints'' de una imagen. Esto se refiere a puntos en una imagen que se diferencian de su entorno y, por lo tanto, el verlos en otra imagen hace que se puedan reconocer.\n",
    "\n",
    "Por ejemplo, el borde de una mesa no es un buen punto debido a que este borde es igual al borde de la mesa un poco más a la izquierda. Sin embargo, las esquinas de la mesa sí son puntos clave, ya que no se parecen a su entorno. Por ello, es necesario escoger los puntos clave de la imagen. \n",
    "\n",
    "Esta búsqueda de puntos clave (keypoints) se calcula en el código del ejercicio con la función de OpenCV \"detectAndCompute\" aplicada a la imagen correspondiente, con el detector de puntos clave creado con la función \"SIFT\\_create()\" de OpenCV.\n",
    "\n",
    "A su vez, la función \"knnMatch\" aplicada a los descriptores de los puntos clave de dos imágenes calcula las mejores coincidencias de puntos clave entre las imágenes. Esto se hace con un comparador de puntos clave de OpenCV que se crea con la función \"FMatcher\".\n",
    "\n",
    "Esto ayuda a comparar imágenes, de modo que cuantas más coincidencias de puntos clave obtengan, más se parecerán. Para ello, se debe hacer una limpieza de puntos, de manera que los puntos con cierta coincidencia pero con una coincidencia parecida con la segunda mejor coincidencia no se debe tener en cuenta, pero los que tienen una coincidencia muy distinta a la segunda mejor coincidencia se tienen en cuenta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d260a8b",
   "metadata": {},
   "source": [
    "## Ejecución del código: Ejemplo de funcionamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3f2e3",
   "metadata": {},
   "source": [
    "Para ejecutar el código de CLASIFICADOR, se debe de ejecutar el código de `clasificador.py` desde la carpeta `CLASIFICADOR + SIFT` en el entorno de anaconda prompt explicado al inicio de la asignatura. Se deben de pasar como parámetros la carpeta en la que se encuentran las imágenes y el nombre del método. Si por ejemplo, se quiere realizar el método *Embedder* con la carpeta de fotografías que se encuentra ejecutando *cd ../images/ImagesEmbedder*, se debe de ejecutar el siguiente comando:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a011b",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">clasificador.py --method=embedder --models=../images/ImagesEmbedder</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1c68e",
   "metadata": {},
   "source": [
    "Es relevante mencionar que se muestra la imagen a la que más se parece el frame actual arriba a la izquierda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa04a5",
   "metadata": {},
   "source": [
    "- **Embedder**:\n",
    "\n",
    "  Se van a presentar algunas imágenes del funcionamiento del clasificador Embedder. Si se utiliza una carpeta con imágenes  (models) de distintas zonas de una habitación (incluso personas) las identifica de zona correcta, aunque la imagen tenga distinta luminancia, resolución, o tamaño que los frames tomados con la cámara actual:\n",
    "\n",
    "  <table style=\"width: 70%\"><tr>\n",
    "  <td> <img src=\"img/embedder-cara.png\"/> </td>\n",
    "  <td> <img src=\"img/embedder-otra-camara-luz.png\"/> </td>\n",
    "  <td> <img src=\"img/embedder-pared.png\"/> </td>\n",
    "  </tr></table>\n",
    "\n",
    "  A su vez, funciona con cuadros, aunque estos se encuentren movidos o con colores algo alterados:\n",
    "\n",
    "  <table style=\"width: 70%\"><tr>\n",
    "  <td> <img src=\"img/cuadro-2.png\"/> </td>\n",
    "  <td> <img src=\"img/cuadro-1.png\"/> </td>\n",
    "  <td> <img src=\"img/cuadros-error.png\"/> </td>\n",
    "  </tr></table>\n",
    "\n",
    "  Como se ha podido observar, el clasificador falla en caso de que la fotografía se encuentre girada y más pequeña.\n",
    "\n",
    "  Por lo tanto, para observar habitaciones funciona de forma correcta, pero con objetos (como cuadros) son preferibles otros clasificadores, a menos que estos objetos se muestren en la misma posición, distancia y ángulo que en la fotografía.\n",
    "\n",
    "  Por último indicar que con objetos simples funciona de forma correcta a pesar de la diferencia de color entre la imagen y el frame, como se puede observar a continuación:\n",
    "\n",
    "  <video src=\"img/objetos-simples-embedder.mp4\" controls='play' style=\"width:40%\">\n",
    "  </video>\n",
    "\n",
    "- **skimageHog**:\n",
    "\n",
    "  En este filtro, hay que tener cuidado con qué objeto se escoge, ya que si se escoge un objeto muy simple, podría confundirse con otras cosas. Por ejemplo, en el siguiente vídeo, en vez de identificar las gafas todo el tiempo, en ocasiones se confunde con el fondo de la imagen al pensar que es un ratón:\n",
    "\n",
    "  <video src=\"img/confunde-gafas-raton.mp4\" controls='play' style=\"width:40%\">\n",
    "  </video>\n",
    "\n",
    "- **sift**:\n",
    "\n",
    "  El método SIFT funciona de forma correcta en cuadros y objetos complejos, pero hay que tener cuidado con los objetos simples, ya que, al tener pocos keypoints, son dificilmente identificables. Un ejemplo es el siguiente vídeo:\n",
    "\n",
    "  <video src=\"img/objetos-simples-sift.mp4\" controls='play' style=\"width:40%\">\n",
    "  </video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4587d5bf",
   "metadata": {},
   "source": [
    "## Tardanza del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e16a34",
   "metadata": {},
   "source": [
    "Para ver cuanto tarda en ejecutar el código se va a añadir las siguientes líneas de código al inicio del programa (antes de mostrar frames), y cuando se captura y cuando se muestra el frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# ...\n",
    "inicio = time.time()\n",
    "\n",
    "# Código que se hace cada frame\n",
    "\n",
    "fin = time.time()\n",
    "print(fin-inicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427f75d",
   "metadata": {},
   "source": [
    "Como se puede observar, este código tarda 0 segundos porque no hace nada en medio, pero al añadirlo a `clasificador.py`, el tiempo aumenta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78506e1",
   "metadata": {},
   "source": [
    "Se obtiene:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a56287",
   "metadata": {},
   "source": [
    "**Inicio** (con 1 carpeta que contiene 4 imágenes de distintos cuadros):\n",
    "- **Embedder**: *0.92* segundos\n",
    "- **skimageHog**: *3.4* segundos\n",
    "- **sift**: *5.8* segundos\n",
    "\n",
    "**Cada frame** (con una carpeta que 4 imágenes de distintas zonas de una habitación):\n",
    "- **Embedder**: Entre *0.01* y *0.02* segundos\n",
    "- **skimageHog**: Entre *0.07* y *0.12* segundos\n",
    "- **sift**: Entre *0.15* y *0.26* segundos\n",
    "\n",
    "Se probó moviendo mucho la cámara, de ahí la diferencia de segundos en cada frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6bf43",
   "metadata": {},
   "source": [
    "Como se ha podido obervar, *Embedder* es el método que menos tarda en ejecutarse, seguido por *skimageHog*, y por último *sift*.\n",
    "\n",
    "Se tarda en iniciar debido a que se realizan tareas al inicio con las imágenes de forma que no se tengan que realizar por cada frame (y de este modo reducir el tiempo que tarda en mostrar cada frame y que vaya más fluido)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f355b",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "[MediaPipe Image Embedder](https://developers.google.com/mediapipe/solutions/vision/image_embedder/python)\n",
    "\n",
    "[Comprobar la existencia de un fichero](https://www.geeksforgeeks.org/python-os-path-exists-method/)\n",
    "\n",
    "[Comprobar la existencia de un directorio](https://www.geeksforgeeks.org/python-os-path-isdir-method/)\n",
    "\n",
    "[Recorrer archivos de un directorio](https://www.codigopiton.com/como-listar-archivos-de-carpeta-en-python/)\n",
    "\n",
    "[Material de la asignatura (código y notebooks)](https://github.com/albertoruiz/umucv/blob/master)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
